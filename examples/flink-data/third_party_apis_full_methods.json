{
  "fullMethodsPaths" : [ {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T copy(T from) {\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference", "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getCause",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getCause",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getCause",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getCause",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getName",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava", "org.apache.flink.util.CompressionUtils.unpackEntry" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}", "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Output.position",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.getCause",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.getCause",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipFile.getEntries",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.<init>",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Input.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.<init>" ],
    "fullMethods" : [ "public NoFetchingInput(InputStream inputStream) {\n    super(inputStream, 8);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.io.compression.XZInputStreamFactory.create",
    "thirdPartyMethod" : "org.apache.commons.compress.compressors.xz.XZCompressorInputStream.<init>",
    "thirdPartyPackage" : "org.apache.commons.compress.compressors.xz",
    "path" : [ "org.apache.flink.api.common.io.compression.XZInputStreamFactory.create" ],
    "fullMethods" : [ "@Override\npublic XZCompressorInputStream create(InputStream in) throws IOException {\n    return new XZCompressorInputStream(in, true);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.KryoUtils.applyRegistrations",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getRegistration",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.KryoUtils.applyRegistrations" ],
    "fullMethods" : [ "/**\n * Apply a list of {@link KryoRegistration} to a Kryo instance. The list of registrations is\n * assumed to already be a final resolution of all possible registration overwrites.\n *\n * <p>The registrations are applied in the given order and always specify the registration id,\n * using the given {@code firstRegistrationId} and incrementing it for each registration.\n *\n * @param kryo\n * \t\tthe Kryo instance to apply the registrations\n * @param resolvedRegistrations\n * \t\tthe registrations, which should already be resolved of all\n * \t\tpossible registration overwrites\n * @param firstRegistrationId\n * \t\tthe first registration id to use\n */\npublic static void applyRegistrations(Kryo kryo, Collection<KryoRegistration> resolvedRegistrations, int firstRegistrationId) {\n    int currentRegistrationId = firstRegistrationId;\n    Serializer<?> serializer;\n    for (KryoRegistration registration : resolvedRegistrations) {\n        serializer = registration.getSerializer(kryo);\n        if (serializer != null) {\n            kryo.register(registration.getRegisteredClass(), serializer, currentRegistrationId);\n        } else {\n            kryo.register(registration.getRegisteredClass(), currentRegistrationId);\n        }\n        // if Kryo already had a serializer for that type then it ignores the registration\n        if (kryo.getRegistration(currentRegistrationId) != null) {\n            currentRegistrationId++;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\npublic TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n    if (!(oldSerializerSnapshot instanceof KryoSerializerSnapshot)) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    KryoSerializerSnapshot<T> oldKryoSerializerSnapshot = ((KryoSerializerSnapshot<T>) (oldSerializerSnapshot));\n    if (snapshotData.getTypeClass() != oldKryoSerializerSnapshot.snapshotData.getTypeClass()) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    return resolveSchemaCompatibility(oldKryoSerializerSnapshot);\n}", "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(KryoSerializerSnapshot<T> oldKryoSerializerSnapshot) {\n    // merge the default serializers\n    final MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializers(), snapshotData.getDefaultKryoSerializers());\n    if (reconfiguredDefaultKryoSerializers.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializers);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge default serializer classes\n    final MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializerClasses(), snapshotData.getDefaultKryoSerializerClasses());\n    if (reconfiguredDefaultKryoSerializerClasses.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializerClasses);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge registration\n    final MergeResult<String, KryoRegistration> reconfiguredRegistrations = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getKryoRegistrations(), snapshotData.getKryoRegistrations());\n    if (reconfiguredRegistrations.hasMissingKeys()) {\n        logMissingKeys(reconfiguredRegistrations);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // there are no missing keys, now we have to decide whether we are compatible as-is or we\n    // require reconfiguration.\n    return resolveSchemaCompatibility(reconfiguredDefaultKryoSerializers, reconfiguredDefaultKryoSerializerClasses, reconfiguredRegistrations);\n}", "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers, MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses, MergeResult<String, KryoRegistration> reconfiguredRegistrations) {\n    if ((reconfiguredDefaultKryoSerializers.isOrderedSubset() && reconfiguredDefaultKryoSerializerClasses.isOrderedSubset()) && reconfiguredRegistrations.isOrderedSubset()) {\n        return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n    // reconfigure a new KryoSerializer\n    KryoSerializer<T> reconfiguredSerializer = new KryoSerializer<>(snapshotData.getTypeClass(), reconfiguredDefaultKryoSerializers.getMerged(), reconfiguredDefaultKryoSerializerClasses.getMerged(), reconfiguredRegistrations.getMerged());\n    return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(reconfiguredSerializer);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\npublic TypeSerializer<T> restoreSerializer() {\n    return new KryoSerializer<>(snapshotData.getTypeClass(), snapshotData.getDefaultKryoSerializers().unwrapOptionals(), snapshotData.getDefaultKryoSerializerClasses().unwrapOptionals(), snapshotData.getKryoRegistrations().unwrapOptionals());\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\n@PublicEvolving\n@SuppressWarnings(\"unchecked\")\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.isForceKryoEnabled()) {\n        return new KryoSerializer<>(getTypeClass(), config);\n    }\n    if (config.isForceAvroEnabled()) {\n        return AvroUtils.getAvroUtils().createAvroSerializer(getTypeClass());\n    }\n    return createPojoSerializer(config);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\n@PublicEvolving\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.hasGenericTypesDisabled()) {\n        throw new UnsupportedOperationException((\"Generic types have been disabled in the ExecutionConfig and type \" + this.typeClass.getName()) + \" is treated as a generic type.\");\n    }\n    return new KryoSerializer<T>(this.typeClass, config);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.Logger.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.minlog.Log",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging", "org.apache.flink.api.java.typeutils.runtime.kryo.MinlogForwarder.<init>" ],
    "fullMethods" : [ "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}", "MinlogForwarder(Logger log) {\n    this.log = checkNotNull(log);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.KryoUtils.applyRegistrations",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.register",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.KryoUtils.applyRegistrations" ],
    "fullMethods" : [ "/**\n * Apply a list of {@link KryoRegistration} to a Kryo instance. The list of registrations is\n * assumed to already be a final resolution of all possible registration overwrites.\n *\n * <p>The registrations are applied in the given order and always specify the registration id,\n * using the given {@code firstRegistrationId} and incrementing it for each registration.\n *\n * @param kryo\n * \t\tthe Kryo instance to apply the registrations\n * @param resolvedRegistrations\n * \t\tthe registrations, which should already be resolved of all\n * \t\tpossible registration overwrites\n * @param firstRegistrationId\n * \t\tthe first registration id to use\n */\npublic static void applyRegistrations(Kryo kryo, Collection<KryoRegistration> resolvedRegistrations, int firstRegistrationId) {\n    int currentRegistrationId = firstRegistrationId;\n    Serializer<?> serializer;\n    for (KryoRegistration registration : resolvedRegistrations) {\n        serializer = registration.getSerializer(kryo);\n        if (serializer != null) {\n            kryo.register(registration.getRegisteredClass(), serializer, currentRegistrationId);\n        } else {\n            kryo.register(registration.getRegisteredClass(), currentRegistrationId);\n        }\n        // if Kryo already had a serializer for that type then it ignores the registration\n        if (kryo.getRegistration(currentRegistrationId) != null) {\n            currentRegistrationId++;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getLinkName",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava", "org.apache.flink.util.CompressionUtils.unpackEntry" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}", "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T copy(T from) {\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference", "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.writeObject",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read", "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require" ],
    "fullMethods" : [ "@Override\npublic int read() throws KryoException {\n    require(1);\n    return buffer[position++] & 0xff;\n}", "/**\n * Require makes sure that at least required number of bytes are kept in the buffer. If not,\n * then it will load exactly the difference between required and currently available number of\n * bytes. Thus, it will only load the data which is required and never prefetch data.\n *\n * @param required\n * \t\tthe number of bytes being available in the buffer\n * @return The number of bytes remaining in the buffer, which will be at least <code>required\n</code> bytes.\n * @throws KryoException\n */\n@Override\nprotected int require(int required) throws KryoException {\n    // The main change between this and Kryo 5 Input.require is this will never read more bytes\n    // than required.\n    // There are also formatting changes to be compliant with the Flink project styling rules.\n    int remaining = limit - position;\n    if (remaining >= required) {\n        return remaining;\n    }\n    if (required > capacity) {\n        throw new KryoException(((\"Buffer too small: capacity: \" + capacity) + \", required: \") + required);\n    }\n    int count;\n    // Try to fill the buffer.\n    if (remaining > 0) {\n        // Logical change 1 (from Kryo Input.require): \"capacity - limit\" -> \"required - limit\"\n        count = fill(buffer, limit, required - limit);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n        if (remaining >= required) {\n            limit += count;\n            return remaining;\n        }\n    }\n    // Was not enough, compact and try again.\n    System.arraycopy(buffer, position, buffer, 0, remaining);\n    total += position;\n    position = 0;\n    do {\n        // Logical change 2 (from Kryo Input.require): \"capacity - remaining\" -> \"required -\n        // remaining\"\n        count = fill(buffer, remaining, required - remaining);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n    } while (remaining < required );\n    limit = remaining;\n    return remaining;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T copy(T from) {\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference",
    "thirdPartyMethod" : "org.objenesis.strategy.StdInstantiatorStrategy.<init>",
    "thirdPartyPackage" : "org.objenesis.strategy",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference", "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.getBestLineBreak",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.getBestLineBreak",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n    if (!(oldSerializerSnapshot instanceof KryoSerializerSnapshot)) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    KryoSerializerSnapshot<T> oldKryoSerializerSnapshot = ((KryoSerializerSnapshot<T>) (oldSerializerSnapshot));\n    if (snapshotData.getTypeClass() != oldKryoSerializerSnapshot.snapshotData.getTypeClass()) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    return resolveSchemaCompatibility(oldKryoSerializerSnapshot);\n}", "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(KryoSerializerSnapshot<T> oldKryoSerializerSnapshot) {\n    // merge the default serializers\n    final MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializers(), snapshotData.getDefaultKryoSerializers());\n    if (reconfiguredDefaultKryoSerializers.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializers);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge default serializer classes\n    final MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializerClasses(), snapshotData.getDefaultKryoSerializerClasses());\n    if (reconfiguredDefaultKryoSerializerClasses.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializerClasses);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge registration\n    final MergeResult<String, KryoRegistration> reconfiguredRegistrations = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getKryoRegistrations(), snapshotData.getKryoRegistrations());\n    if (reconfiguredRegistrations.hasMissingKeys()) {\n        logMissingKeys(reconfiguredRegistrations);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // there are no missing keys, now we have to decide whether we are compatible as-is or we\n    // require reconfiguration.\n    return resolveSchemaCompatibility(reconfiguredDefaultKryoSerializers, reconfiguredDefaultKryoSerializerClasses, reconfiguredRegistrations);\n}", "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers, MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses, MergeResult<String, KryoRegistration> reconfiguredRegistrations) {\n    if ((reconfiguredDefaultKryoSerializers.isOrderedSubset() && reconfiguredDefaultKryoSerializerClasses.isOrderedSubset()) && reconfiguredRegistrations.isOrderedSubset()) {\n        return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n    // reconfigure a new KryoSerializer\n    KryoSerializer<T> reconfiguredSerializer = new KryoSerializer<>(snapshotData.getTypeClass(), reconfiguredDefaultKryoSerializers.getMerged(), reconfiguredDefaultKryoSerializerClasses.getMerged(), reconfiguredRegistrations.getMerged());\n    return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(reconfiguredSerializer);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic TypeSerializer<T> restoreSerializer() {\n    return new KryoSerializer<>(snapshotData.getTypeClass(), snapshotData.getDefaultKryoSerializers().unwrapOptionals(), snapshotData.getDefaultKryoSerializerClasses().unwrapOptionals(), snapshotData.getKryoRegistrations().unwrapOptionals());\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\n@PublicEvolving\n@SuppressWarnings(\"unchecked\")\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.isForceKryoEnabled()) {\n        return new KryoSerializer<>(getTypeClass(), config);\n    }\n    if (config.isForceAvroEnabled()) {\n        return AvroUtils.getAvroUtils().createAvroSerializer(getTypeClass());\n    }\n    return createPojoSerializer(config);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\n@PublicEvolving\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.hasGenericTypesDisabled()) {\n        throw new UnsupportedOperationException((\"Generic types have been disabled in the ExecutionConfig and type \" + this.typeClass.getName()) + \" is treated as a generic type.\");\n    }\n    return new KryoSerializer<T>(this.typeClass, config);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.setLogger",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.KryoBufferUnderflowException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read", "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require" ],
    "fullMethods" : [ "@Override\npublic int read() throws KryoException {\n    require(1);\n    return buffer[position++] & 0xff;\n}", "/**\n * Require makes sure that at least required number of bytes are kept in the buffer. If not,\n * then it will load exactly the difference between required and currently available number of\n * bytes. Thus, it will only load the data which is required and never prefetch data.\n *\n * @param required\n * \t\tthe number of bytes being available in the buffer\n * @return The number of bytes remaining in the buffer, which will be at least <code>required\n</code> bytes.\n * @throws KryoException\n */\n@Override\nprotected int require(int required) throws KryoException {\n    // The main change between this and Kryo 5 Input.require is this will never read more bytes\n    // than required.\n    // There are also formatting changes to be compliant with the Flink project styling rules.\n    int remaining = limit - position;\n    if (remaining >= required) {\n        return remaining;\n    }\n    if (required > capacity) {\n        throw new KryoException(((\"Buffer too small: capacity: \" + capacity) + \", required: \") + required);\n    }\n    int count;\n    // Try to fill the buffer.\n    if (remaining > 0) {\n        // Logical change 1 (from Kryo Input.require): \"capacity - limit\" -> \"required - limit\"\n        count = fill(buffer, limit, required - limit);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n        if (remaining >= required) {\n            limit += count;\n            return remaining;\n        }\n    }\n    // Was not enough, compact and try again.\n    System.arraycopy(buffer, position, buffer, 0, remaining);\n    total += position;\n    position = 0;\n    do {\n        // Logical change 2 (from Kryo Input.require): \"capacity - remaining\" -> \"required -\n        // remaining\"\n        count = fill(buffer, remaining, required - remaining);\n        if (count == (-1)) {\n            throw new KryoBufferUnderflowException(\"Buffer underflow.\");\n        }\n        remaining += count;\n    } while (remaining < required );\n    limit = remaining;\n    return remaining;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.ObjectMap.get",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.ObjectMap.get",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.schema.CoreSchema.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.schema",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveEntry.isDirectory",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava", "org.apache.flink.util.CompressionUtils.unpackEntry" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}", "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.PojoTypeInfo.toString",
    "thirdPartyMethod" : "org.apache.commons.lang3.StringUtils.join",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.PojoTypeInfo.toString" ],
    "fullMethods" : [ "@Override\npublic String toString() {\n    List<String> fieldStrings = new ArrayList<String>();\n    for (PojoField field : fields) {\n        fieldStrings.add((field.getField().getName() + \": \") + field.getTypeInformation().toString());\n    }\n    return ((((\"PojoType<\" + getTypeClass().getName()) + \", fields = [\") + StringUtils.join(fieldStrings, \", \")) + \"]\") + \">\";\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Output.reset",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getClassLoader",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSplitLines",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.loadFromString",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContext",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContext",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContext",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContext",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setReferences",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setReferences",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setReferences",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setReferences",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setReferences",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setReferences",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblemMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblemMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblemMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblemMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveEntry.isSymbolicLink",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava", "org.apache.flink.util.CompressionUtils.unpackEntry" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}", "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T copy(T from) {\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy.setFallbackInstantiatorStrategy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference", "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipFile.getInputStream",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblem",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblem",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblem",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getProblem",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setDefaultFlowStyle",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter.<init>",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.representer.StandardRepresenter.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.representer",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.FlinkConfigRepresenter.<init>" ],
    "fullMethods" : [ "public FlinkConfigRepresenter(DumpSettings dumpSettings) {\n    super(dumpSettings);\n    representers.put(Duration.class, this::representDuration);\n    representers.put(MemorySize.class, this::representMemorySize);\n    parentClassRepresenters.put(Enum.class, this::representEnum);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getLine",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getLine",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getLine",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getLine",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.dumpToString",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.dumpToString",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.ExecutionConfig.configure",
    "thirdPartyMethod" : "org.apache.commons.compress.utils.Sets.newHashSet",
    "thirdPartyPackage" : "org.apache.commons.compress.utils",
    "path" : [ "org.apache.flink.api.common.ExecutionConfig.configure", "org.apache.flink.configuration.RestartStrategyOptions.<clinit>", "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>" ],
    "fullMethods" : [ "/**\n * Sets all relevant options contained in the {@link ReadableConfig} such as e.g. {@link PipelineOptions#CLOSURE_CLEANER_LEVEL}.\n *\n * <p>It will change the value of a setting only if a corresponding option was set in the {@code configuration}. If a key is not present, the current value of a field will remain untouched.\n *\n * @param configuration\n * \t\ta configuration to read the values from\n * @param classLoader\n * \t\ta class loader to use when loading classes\n */\npublic void configure(ReadableConfig configuration, ClassLoader classLoader) {\n    configuration.getOptional(PipelineOptions.AUTO_GENERATE_UIDS).ifPresent(this::setAutoGeneratedUids);\n    configuration.getOptional(PipelineOptions.AUTO_WATERMARK_INTERVAL).ifPresent(this::setAutoWatermarkInterval);\n    configuration.getOptional(PipelineOptions.CLOSURE_CLEANER_LEVEL).ifPresent(this::setClosureCleanerLevel);\n    configuration.getOptional(PipelineOptions.GLOBAL_JOB_PARAMETERS).ifPresent(this::setGlobalJobParameters);\n    configuration.getOptional(MetricOptions.LATENCY_INTERVAL).ifPresent(interval -> setLatencyTrackingInterval(interval.toMillis()));\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_ENABLED).ifPresent(this::enablePeriodicMaterialize);\n    configuration.getOptional(StateChangelogOptions.PERIODIC_MATERIALIZATION_INTERVAL).ifPresent(this::setPeriodicMaterializeIntervalMillis);\n    configuration.getOptional(StateChangelogOptions.MATERIALIZATION_MAX_FAILURES_ALLOWED).ifPresent(this::setMaterializationMaxAllowedFailures);\n    configuration.getOptional(PipelineOptions.MAX_PARALLELISM).ifPresent(this::setMaxParallelism);\n    configuration.getOptional(CoreOptions.DEFAULT_PARALLELISM).ifPresent(this::setParallelism);\n    configuration.getOptional(PipelineOptions.OBJECT_REUSE).ifPresent(this::setObjectReuse);\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_INTERVAL).ifPresent(interval -> setTaskCancellationInterval(interval.toMillis()));\n    configuration.getOptional(TaskManagerOptions.TASK_CANCELLATION_TIMEOUT).ifPresent(timeout -> setTaskCancellationTimeout(timeout.toMillis()));\n    configuration.getOptional(ExecutionOptions.SNAPSHOT_COMPRESSION).ifPresent(this::setUseSnapshotCompression);\n    configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY).ifPresent(s -> this.setRestartStrategy(configuration));\n    configuration.getOptional(JobManagerOptions.SCHEDULER).ifPresent(t -> this.configuration.set(JobManagerOptions.SCHEDULER, t));\n    serializerConfig.configure(configuration, classLoader);\n}", "", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.values",
    "thirdPartyMethod" : "org.apache.commons.compress.utils.Sets.newHashSet",
    "thirdPartyPackage" : "org.apache.commons.compress.utils",
    "path" : [ "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.values", "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>" ],
    "fullMethods" : [ "{\n    java.lang.Object $stack1;\n    org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType[] $stack0, $stack2;\n\n\n    $stack0 = <org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType: org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType[] $VALUES>;\n    $stack1 = virtualinvoke $stack0.<java.lang.Object: java.lang.Object clone()>();\n    $stack2 = (org.apache.flink.configuration.RestartStrategyOptions$RestartStrategyType[]) $stack1;\n\n    return $stack2;\n}\n", "" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.RestartStrategyDescriptionUtils.getRestartStrategyDescription",
    "thirdPartyMethod" : "org.apache.commons.compress.utils.Sets.newHashSet",
    "thirdPartyPackage" : "org.apache.commons.compress.utils",
    "path" : [ "org.apache.flink.api.common.RestartStrategyDescriptionUtils.getRestartStrategyDescription", "org.apache.flink.configuration.RestartStrategyOptions.<clinit>", "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>" ],
    "fullMethods" : [ "/**\n * Returns a descriptive string of the restart strategy configured in the given Configuration\n * object.\n *\n * @param configuration\n * \t\tthe Configuration to extract the restart strategy from\n * @return a description of the restart strategy\n */\npublic static String getRestartStrategyDescription(Configuration configuration) {\n    final Optional<String> restartStrategyNameOptional = configuration.getOptional(RestartStrategyOptions.RESTART_STRATEGY);\n    return restartStrategyNameOptional.map(restartStrategyName -> {\n        switch (RestartStrategyOptions.RestartStrategyType.of(restartStrategyName.toLowerCase())) {\n            case NO_RESTART_STRATEGY :\n                return \"Restart deactivated.\";\n            case FIXED_DELAY :\n                return getFixedDelayDescription(configuration);\n            case FAILURE_RATE :\n                return getFailureRateDescription(configuration);\n            case EXPONENTIAL_DELAY :\n                return getExponentialDelayDescription(configuration);\n            default :\n                throw new IllegalArgumentException((\"Unknown restart strategy \" + restartStrategyName) + \".\");\n        }\n    }).orElse(\"Cluster level default restart strategy\");\n}", "", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.of",
    "thirdPartyMethod" : "org.apache.commons.compress.utils.Sets.newHashSet",
    "thirdPartyPackage" : "org.apache.commons.compress.utils",
    "path" : [ "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.of", "org.apache.flink.configuration.RestartStrategyOptions.RestartStrategyType.<clinit>" ],
    "fullMethods" : [ "/**\n * Return the corresponding RestartStrategyType based on the displayed value.\n */\npublic static RestartStrategyType of(String value) {\n    for (RestartStrategyType restartStrategyType : RestartStrategyType.values()) {\n        if (restartStrategyType.getAllAvailableValues().contains(value)) {\n            return restartStrategyType;\n        }\n    }\n    throw new IllegalArgumentException(String.format(\"%s is an unknown value of RestartStrategyType.\", value));\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getColumn",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getColumn",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getColumn",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getColumn",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipFile.close",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.operators.Keys.ExpressionKeys.toString",
    "thirdPartyMethod" : "org.apache.commons.lang3.StringUtils.join",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.common.operators.Keys.ExpressionKeys.toString" ],
    "fullMethods" : [ "@Override\npublic String toString() {\n    return \"ExpressionKeys: \" + StringUtils.join(keyFields, '.');\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipFile.<init>",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.<init>",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Serializer.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.<init>" ],
    "fullMethods" : [ "public JavaSerializer() {\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.DummyAvroKryoSerializerClass.<init>",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Serializer.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.Serializers.DummyAvroKryoSerializerClass.<init>" ],
    "fullMethods" : [ "DummyAvroKryoSerializerClass() {\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettingsBuilder.setSchema",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContextMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContextMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContextMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.MarkedYamlEngineException.getContextMark",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Output.flush",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.ParameterTool.fromArgs",
    "thirdPartyMethod" : "org.apache.commons.lang3.math.NumberUtils.isNumber",
    "thirdPartyPackage" : "org.apache.commons.lang3.math",
    "path" : [ "org.apache.flink.util.ParameterTool.fromArgs" ],
    "fullMethods" : [ "// ------------------ Constructors ------------------------\n/**\n * Returns {@link ParameterTool} for the given arguments. The arguments are keys followed by\n * values. Keys have to start with '-' or '--'\n *\n * <p><strong>Example arguments:</strong> --key1 value1 --key2 value2 -key3 value3\n *\n * @param args\n * \t\tInput array arguments\n * @return A {@link ParameterTool}\n */\npublic static ParameterTool fromArgs(String[] args) {\n    final Map<String, String> map = CollectionUtil.newHashMapWithExpectedSize(args.length / 2);\n    int i = 0;\n    while (i < args.length) {\n        final String key = Utils.getKeyFromArgs(args, i);\n        if (key.isEmpty()) {\n            throw new IllegalArgumentException((\"The input \" + Arrays.toString(args)) + \" contains an empty argument\");\n        }\n        i += 1;// try to find the value\n\n        if (i >= args.length) {\n            map.put(key, AbstractParameterTool.NO_VALUE_KEY);\n        } else if (NumberUtils.isNumber(args[i])) {\n            map.put(key, args[i]);\n            i += 1;\n        } else if (args[i].startsWith(\"--\") || args[i].startsWith(\"-\")) {\n            // the argument cannot be a negative number because we checked earlier\n            // -> the next argument is a parameter name\n            map.put(key, AbstractParameterTool.NO_VALUE_KEY);\n        } else {\n            map.put(key, args[i]);\n            i += 1;\n        }\n    } \n    return fromMap(map);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.MultipleParameterTool.fromArgs",
    "thirdPartyMethod" : "org.apache.commons.lang3.math.NumberUtils.isNumber",
    "thirdPartyPackage" : "org.apache.commons.lang3.math",
    "path" : [ "org.apache.flink.util.MultipleParameterTool.fromArgs" ],
    "fullMethods" : [ "// ------------------ Constructors ------------------------\n/**\n * Returns {@link MultipleParameterTool} for the given arguments. The arguments are keys\n * followed by values. Keys have to start with '-' or '--'\n *\n * <p><strong>Example arguments:</strong> --key1 value1 --key2 value2 -key3 value3 --multi\n * multiValue1 --multi multiValue2\n *\n * @param args\n * \t\tInput array arguments\n * @return A {@link MultipleParameterTool}\n */\npublic static MultipleParameterTool fromArgs(String[] args) {\n    final Map<String, Collection<String>> map = CollectionUtil.newHashMapWithExpectedSize(args.length / 2);\n    int i = 0;\n    while (i < args.length) {\n        final String key = Utils.getKeyFromArgs(args, i);\n        i += 1;// try to find the value\n\n        map.putIfAbsent(key, new ArrayList<>());\n        if (i >= args.length) {\n            map.get(key).add(NO_VALUE_KEY);\n        } else if (NumberUtils.isNumber(args[i])) {\n            map.get(key).add(args[i]);\n            i += 1;\n        } else if (args[i].startsWith(\"--\") || args[i].startsWith(\"-\")) {\n            // the argument cannot be a negative number because we checked earlier\n            // -> the next argument is a parameter name\n            map.get(key).add(NO_VALUE_KEY);\n        } else {\n            map.get(key).add(args[i]);\n            i += 1;\n        }\n    } \n    return fromMultiMap(map);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getName",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getName",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getName",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.Mark.getName",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.isDirectory",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.newInstance",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.getUnixMode",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.KryoBufferUnderflowException.getMessage",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Output.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Output.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Input.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.resolveSchemaCompatibility", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n    if (!(oldSerializerSnapshot instanceof KryoSerializerSnapshot)) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    KryoSerializerSnapshot<T> oldKryoSerializerSnapshot = ((KryoSerializerSnapshot<T>) (oldSerializerSnapshot));\n    if (snapshotData.getTypeClass() != oldKryoSerializerSnapshot.snapshotData.getTypeClass()) {\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    return resolveSchemaCompatibility(oldKryoSerializerSnapshot);\n}", "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(KryoSerializerSnapshot<T> oldKryoSerializerSnapshot) {\n    // merge the default serializers\n    final MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializers(), snapshotData.getDefaultKryoSerializers());\n    if (reconfiguredDefaultKryoSerializers.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializers);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge default serializer classes\n    final MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getDefaultKryoSerializerClasses(), snapshotData.getDefaultKryoSerializerClasses());\n    if (reconfiguredDefaultKryoSerializerClasses.hasMissingKeys()) {\n        logMissingKeys(reconfiguredDefaultKryoSerializerClasses);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // merge registration\n    final MergeResult<String, KryoRegistration> reconfiguredRegistrations = mergeRightIntoLeft(oldKryoSerializerSnapshot.snapshotData.getKryoRegistrations(), snapshotData.getKryoRegistrations());\n    if (reconfiguredRegistrations.hasMissingKeys()) {\n        logMissingKeys(reconfiguredRegistrations);\n        return TypeSerializerSchemaCompatibility.incompatible();\n    }\n    // there are no missing keys, now we have to decide whether we are compatible as-is or we\n    // require reconfiguration.\n    return resolveSchemaCompatibility(reconfiguredDefaultKryoSerializers, reconfiguredDefaultKryoSerializerClasses, reconfiguredRegistrations);\n}", "private TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(MergeResult<Class<?>, SerializableSerializer<?>> reconfiguredDefaultKryoSerializers, MergeResult<Class<?>, Class<? extends Serializer<?>>> reconfiguredDefaultKryoSerializerClasses, MergeResult<String, KryoRegistration> reconfiguredRegistrations) {\n    if ((reconfiguredDefaultKryoSerializers.isOrderedSubset() && reconfiguredDefaultKryoSerializerClasses.isOrderedSubset()) && reconfiguredRegistrations.isOrderedSubset()) {\n        return TypeSerializerSchemaCompatibility.compatibleAsIs();\n    }\n    // reconfigure a new KryoSerializer\n    KryoSerializer<T> reconfiguredSerializer = new KryoSerializer<>(snapshotData.getTypeClass(), reconfiguredDefaultKryoSerializers.getMerged(), reconfiguredDefaultKryoSerializerClasses.getMerged(), reconfiguredRegistrations.getMerged());\n    return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(reconfiguredSerializer);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "// ------------------------------------------------------------------------\npublic KryoSerializer(Class<T> type, SerializerConfig serializerConfig) {\n    this.type = checkNotNull(type);\n    this.defaultSerializers = ((SerializerConfigImpl) (serializerConfig)).getDefaultKryoSerializers();\n    this.defaultSerializerClasses = serializerConfig.getDefaultKryoSerializerClasses();\n    this.kryoRegistrations = buildKryoRegistrations(this.type, serializerConfig.getRegisteredKryoTypes(), serializerConfig.getRegisteredTypesWithKryoSerializerClasses(), ((SerializerConfigImpl) (serializerConfig)).getRegisteredTypesWithKryoSerializers(), serializerConfig.isForceKryoAvroEnabled());\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializerSnapshot.restoreSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic TypeSerializer<T> restoreSerializer() {\n    return new KryoSerializer<>(snapshotData.getTypeClass(), snapshotData.getDefaultKryoSerializers().unwrapOptionals(), snapshotData.getDefaultKryoSerializerClasses().unwrapOptionals(), snapshotData.getKryoRegistrations().unwrapOptionals());\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.PojoTypeInfo.createSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\n@PublicEvolving\n@SuppressWarnings(\"unchecked\")\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.isForceKryoEnabled()) {\n        return new KryoSerializer<>(getTypeClass(), config);\n    }\n    if (config.isForceAvroEnabled()) {\n        return AvroUtils.getAvroUtils().createAvroSerializer(getTypeClass());\n    }\n    return createPojoSerializer(config);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.GenericTypeInfo.createSerializer", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\n@PublicEvolving\npublic TypeSerializer<T> createSerializer(SerializerConfig config) {\n    if (config.hasGenericTypesDisabled()) {\n        throw new UnsupportedOperationException((\"Generic types have been disabled in the ExecutionConfig and type \" + this.typeClass.getName()) + \" is treated as a generic type.\");\n    }\n    return new KryoSerializer<T>(this.typeClass, config);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate",
    "thirdPartyMethod" : "com.esotericsoftware.minlog.Log.TRACE",
    "thirdPartyPackage" : "com.esotericsoftware.minlog",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<clinit>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.configureKryoLogging" ],
    "fullMethods" : [ "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}", "", "static void configureKryoLogging() {\n    // Kryo uses only DEBUG and TRACE levels\n    // we only forward TRACE level, because even DEBUG levels results in\n    // a logging for each object, which is infeasible in Flink.\n    if (LOG.isTraceEnabled()) {\n        Log.setLogger(new MinlogForwarder(LOG));\n        Log.TRACE();\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T copy(T from) {\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getNextRegistrationId",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.io.Output.close",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.io",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.setStackTrace",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.getSerializer",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.SerializerFactory.ReflectionSerializerFactory.newSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.SerializerFactory",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.KryoRegistration.getSerializer" ],
    "fullMethods" : [ "public Serializer<?> getSerializer(Kryo kryo) {\n    switch (serializerDefinitionType) {\n        case UNSPECIFIED :\n            return null;\n        case CLASS :\n            return ReflectionSerializerFactory.newSerializer(kryo, serializerClass, registeredClass);\n        case INSTANCE :\n            return serializableSerializerInstance.getSerializer();\n        default :\n            // this should not happen; adding as a guard for the future\n            throw new IllegalStateException(\"Unrecognized Kryo registration serializer definition type: \" + serializerDefinitionType);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.LoadSettingsBuilder.build",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Dump.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.getInputFormatTypes",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.getInputFormatTypes", "org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@PublicEvolving\npublic static <IN> TypeInformation<IN> getInputFormatTypes(InputFormat<IN, ?> inputFormatInterface) {\n    if (inputFormatInterface instanceof ResultTypeQueryable) {\n        return ((ResultTypeQueryable<IN>) (inputFormatInterface)).getProducedType();\n    }\n    return new TypeExtractor().privateCreateTypeInfo(InputFormat.class, inputFormatInterface.getClass(), 0, null, null);\n}", "// for (Rich)Functions\n@SuppressWarnings(\"unchecked\")\nprivate <IN1, IN2, OUT> TypeInformation<OUT> privateCreateTypeInfo(Class<?> baseClass, Class<?> clazz, int returnParamPos, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    List<Type> typeHierarchy = new ArrayList<>();\n    Type returnType = getParameterType(baseClass, typeHierarchy, clazz, returnParamPos);\n    TypeInformation<OUT> typeInfo;\n    // return type is a variable -> try to get the type info from the input directly\n    if (returnType instanceof TypeVariable<?>) {\n        typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (returnType)), typeHierarchy, in1Type, in2Type)));\n        if (typeInfo != null) {\n            return typeInfo;\n        }\n    }\n    // get info from hierarchy\n    return createTypeInfoWithTypeHierarchy(typeHierarchy, returnType, in1Type, in2Type);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <IN1, IN2, OUT> TypeInformation<OUT> createTypeInfoWithTypeHierarchy(List<Type> typeHierarchy, Type t, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    // check if type information can be created using a type factory\n    final TypeInformation<OUT> typeFromFactory = createTypeInfoFromFactory(t, typeHierarchy, in1Type, in2Type);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    } else if (isClassType(t) && Tuple.class.isAssignableFrom(typeToClass(t))) {\n        Type curT = t;\n        // do not allow usage of Tuple as type\n        if (typeToClass(t).equals(Tuple.class)) {\n            throw new InvalidTypesException(\"Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead.\");\n        }\n        // go up the hierarchy until we reach immediate child of Tuple (with or without\n        // generics)\n        // collect the types while moving up for a later top-down\n        List<Type> typeHierarchyForSubtypes = new ArrayList<>(typeHierarchy);\n        while (!(isClassType(curT) && typeToClass(curT).getSuperclass().equals(Tuple.class))) {\n            typeHierarchyForSubtypes.add(curT);\n            curT = typeToClass(curT).getGenericSuperclass();\n        } \n        if (curT == Tuple0.class) {\n            return new TupleTypeInfo(Tuple0.class);\n        }\n        // check if immediate child of Tuple has generics\n        if (curT instanceof Class<?>) {\n            throw new InvalidTypesException(\"Tuple needs to be parameterized by using generics.\");\n        }\n        typeHierarchyForSubtypes.add(curT);\n        // create the type information for the subtypes\n        final TypeInformation<?>[] subTypesInfo = createSubTypesInfo(t, ((ParameterizedType) (curT)), typeHierarchyForSubtypes, in1Type, in2Type, false);\n        // type needs to be treated a pojo due to additional fields\n        if (subTypesInfo == null) {\n            return analyzePojo(t, new ArrayList<>(typeHierarchy), in1Type, in2Type);\n        }\n        for (int i = 0; i < subTypesInfo.length; i++) {\n            if (subTypesInfo[i] instanceof GenericTypeInfo) {\n                LOG.info(\"Tuple field #{} of type '{}' will be processed as GenericType. {}\", i + 1, subTypesInfo[i].getTypeClass().getSimpleName(), GENERIC_TYPE_DOC_HINT);\n            }\n        }\n        // return tuple info\n        return new TupleTypeInfo(typeToClass(t), subTypesInfo);\n    } else if (t instanceof TypeVariable) {\n        Type typeVar = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (t)));\n        if (!(typeVar instanceof TypeVariable)) {\n            return createTypeInfoWithTypeHierarchy(typeHierarchy, typeVar, in1Type, in2Type);\n        } else // try to derive the type info of the TypeVariable from the immediate base child input\n        // as a last attempt\n        {\n            TypeInformation<OUT> typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (t)), typeHierarchy, in1Type, in2Type)));\n            if (typeInfo != null) {\n                return typeInfo;\n            } else {\n                throw new InvalidTypesException(((((((\"Type of TypeVariable '\" + ((TypeVariable<?>) (t)).getName()) + \"' in '\") + ((TypeVariable<?>) (t)).getGenericDeclaration()) + \"' could not be determined. This is most likely a type erasure problem. \") + \"The type extraction currently supports types with generic variables only in cases where \") + \"all variables in the return type can be deduced from the input type(s). \") + \"Otherwise the type has to be specified explicitly using type information.\");\n            }\n        }\n    } else if (t instanceof GenericArrayType) {\n        GenericArrayType genericArray = ((GenericArrayType) (t));\n        Type componentType = genericArray.getGenericComponentType();\n        // due to a Java 6 bug, it is possible that the JVM classifies e.g. String[] or int[] as\n        // GenericArrayType instead of Class\n        if (componentType instanceof Class) {\n            Class<?> componentClass = ((Class<?>) (componentType));\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return getForClass(classArray);\n        } else {\n            TypeInformation<?> componentInfo = createTypeInfoWithTypeHierarchy(typeHierarchy, genericArray.getGenericComponentType(), in1Type, in2Type);\n            Class<?> componentClass = componentInfo.getTypeClass();\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return ObjectArrayTypeInfo.getInfoFor(classArray, componentInfo);\n        }\n    } else if (t instanceof ParameterizedType) {\n        return privateGetForClass(typeToClass(t), typeHierarchy, ((ParameterizedType) (t)), in1Type, in2Type);\n    } else if (t instanceof Class) {\n        return privateGetForClass(((Class<OUT>) (t)), typeHierarchy);\n    }\n    throw new InvalidTypesException(\"Type Information could not be created.\");\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "public static TypeInformation<?> createTypeInfo(Type t) {\n    TypeInformation<?> ti = new TypeExtractor().privateCreateTypeInfo(t);\n    if (ti == null) {\n        throw new InvalidTypesException(\"Could not extract type information.\");\n    }\n    return ti;\n}", "// ----------------------------------- private methods ----------------------------------------\nprivate TypeInformation<?> privateCreateTypeInfo(Type t) {\n    List<Type> typeHierarchy = new ArrayList<>();\n    typeHierarchy.add(t);\n    return createTypeInfoWithTypeHierarchy(typeHierarchy, t, null, null);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <IN1, IN2, OUT> TypeInformation<OUT> createTypeInfoWithTypeHierarchy(List<Type> typeHierarchy, Type t, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    // check if type information can be created using a type factory\n    final TypeInformation<OUT> typeFromFactory = createTypeInfoFromFactory(t, typeHierarchy, in1Type, in2Type);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    } else if (isClassType(t) && Tuple.class.isAssignableFrom(typeToClass(t))) {\n        Type curT = t;\n        // do not allow usage of Tuple as type\n        if (typeToClass(t).equals(Tuple.class)) {\n            throw new InvalidTypesException(\"Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead.\");\n        }\n        // go up the hierarchy until we reach immediate child of Tuple (with or without\n        // generics)\n        // collect the types while moving up for a later top-down\n        List<Type> typeHierarchyForSubtypes = new ArrayList<>(typeHierarchy);\n        while (!(isClassType(curT) && typeToClass(curT).getSuperclass().equals(Tuple.class))) {\n            typeHierarchyForSubtypes.add(curT);\n            curT = typeToClass(curT).getGenericSuperclass();\n        } \n        if (curT == Tuple0.class) {\n            return new TupleTypeInfo(Tuple0.class);\n        }\n        // check if immediate child of Tuple has generics\n        if (curT instanceof Class<?>) {\n            throw new InvalidTypesException(\"Tuple needs to be parameterized by using generics.\");\n        }\n        typeHierarchyForSubtypes.add(curT);\n        // create the type information for the subtypes\n        final TypeInformation<?>[] subTypesInfo = createSubTypesInfo(t, ((ParameterizedType) (curT)), typeHierarchyForSubtypes, in1Type, in2Type, false);\n        // type needs to be treated a pojo due to additional fields\n        if (subTypesInfo == null) {\n            return analyzePojo(t, new ArrayList<>(typeHierarchy), in1Type, in2Type);\n        }\n        for (int i = 0; i < subTypesInfo.length; i++) {\n            if (subTypesInfo[i] instanceof GenericTypeInfo) {\n                LOG.info(\"Tuple field #{} of type '{}' will be processed as GenericType. {}\", i + 1, subTypesInfo[i].getTypeClass().getSimpleName(), GENERIC_TYPE_DOC_HINT);\n            }\n        }\n        // return tuple info\n        return new TupleTypeInfo(typeToClass(t), subTypesInfo);\n    } else if (t instanceof TypeVariable) {\n        Type typeVar = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (t)));\n        if (!(typeVar instanceof TypeVariable)) {\n            return createTypeInfoWithTypeHierarchy(typeHierarchy, typeVar, in1Type, in2Type);\n        } else // try to derive the type info of the TypeVariable from the immediate base child input\n        // as a last attempt\n        {\n            TypeInformation<OUT> typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (t)), typeHierarchy, in1Type, in2Type)));\n            if (typeInfo != null) {\n                return typeInfo;\n            } else {\n                throw new InvalidTypesException(((((((\"Type of TypeVariable '\" + ((TypeVariable<?>) (t)).getName()) + \"' in '\") + ((TypeVariable<?>) (t)).getGenericDeclaration()) + \"' could not be determined. This is most likely a type erasure problem. \") + \"The type extraction currently supports types with generic variables only in cases where \") + \"all variables in the return type can be deduced from the input type(s). \") + \"Otherwise the type has to be specified explicitly using type information.\");\n            }\n        }\n    } else if (t instanceof GenericArrayType) {\n        GenericArrayType genericArray = ((GenericArrayType) (t));\n        Type componentType = genericArray.getGenericComponentType();\n        // due to a Java 6 bug, it is possible that the JVM classifies e.g. String[] or int[] as\n        // GenericArrayType instead of Class\n        if (componentType instanceof Class) {\n            Class<?> componentClass = ((Class<?>) (componentType));\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return getForClass(classArray);\n        } else {\n            TypeInformation<?> componentInfo = createTypeInfoWithTypeHierarchy(typeHierarchy, genericArray.getGenericComponentType(), in1Type, in2Type);\n            Class<?> componentClass = componentInfo.getTypeClass();\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return ObjectArrayTypeInfo.getInfoFor(classArray, componentInfo);\n        }\n    } else if (t instanceof ParameterizedType) {\n        return privateGetForClass(typeToClass(t), typeHierarchy, ((ParameterizedType) (t)), in1Type, in2Type);\n    } else if (t instanceof Class) {\n        return privateGetForClass(((Class<OUT>) (t)), typeHierarchy);\n    }\n    throw new InvalidTypesException(\"Type Information could not be created.\");\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.getBinaryOperatorReturnType",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.getBinaryOperatorReturnType", "org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "/**\n * Returns the binary operator's return type.\n *\n * <p>This method can extract a type in 4 different ways:\n *\n * <p>1. By using the generics of the base class like MyFunction<X, Y, Z, IN, OUT>. This is what\n * outputTypeArgumentIndex (in this example \"4\") is good for.\n *\n * <p>2. By using input type inference SubMyFunction<T, String, String, String, T>. This is what\n * inputTypeArgumentIndex (in this example \"0\") and inType is good for.\n *\n * <p>3. By using the static method that a compiler generates for Java lambdas. This is what\n * lambdaOutputTypeArgumentIndices is good for. Given that MyFunction has the following single\n * abstract method:\n *\n * <pre>\n * <code>\n * void apply(IN value, Collector<OUT> value)\n * </code>\n * </pre>\n *\n * <p>Lambda type indices allow the extraction of a type from lambdas. To extract the output\n * type <b>OUT</b> from the function one should pass {@code new int[] {1, 0}}. \"1\" for selecting\n * the parameter and 0 for the first generic in this type. Use {@code TypeExtractor.NO_INDEX}\n * for selecting the return type of the lambda for extraction or if the class cannot be a lambda\n * because it is not a single abstract method interface.\n *\n * <p>4. By using interfaces such as {@link TypeInfoFactory} or {@link ResultTypeQueryable}.\n *\n * <p>See also comments in the header of this class.\n *\n * @param function\n * \t\tFunction to extract the return type from\n * @param baseClass\n * \t\tBase class of the function\n * @param input1TypeArgumentIndex\n * \t\tIndex of first input generic type in the class specification\n * \t\t(ignored if in1Type is null)\n * @param input2TypeArgumentIndex\n * \t\tIndex of second input generic type in the class specification\n * \t\t(ignored if in2Type is null)\n * @param outputTypeArgumentIndex\n * \t\tIndex of output generic type in the class specification\n * @param lambdaOutputTypeArgumentIndices\n * \t\tTable of indices of the type argument specifying the\n * \t\toutput type. See example.\n * @param in1Type\n * \t\tType of the left side input elements (In case of an iterable, it is the\n * \t\telement type)\n * @param in2Type\n * \t\tType of the right side input elements (In case of an iterable, it is the\n * \t\telement type)\n * @param functionName\n * \t\tFunction name\n * @param allowMissing\n * \t\tCan the type information be missing (this generates a MissingTypeInfo for\n * \t\tpostponing an exception)\n * @param <IN1>\n * \t\tLeft side input type\n * @param <IN2>\n * \t\tRight side input type\n * @param <OUT>\n * \t\tOutput type\n * @return TypeInformation of the return type of the function\n */\n@SuppressWarnings(\"unchecked\")\n@PublicEvolving\npublic static <IN1, IN2, OUT> TypeInformation<OUT> getBinaryOperatorReturnType(Function function, Class<?> baseClass, int input1TypeArgumentIndex, int input2TypeArgumentIndex, int outputTypeArgumentIndex, int[] lambdaOutputTypeArgumentIndices, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type, String functionName, boolean allowMissing) {\n    Preconditions.checkArgument((in1Type == null) || (input1TypeArgumentIndex >= 0), \"Input 1 type argument index was not provided\");\n    Preconditions.checkArgument((in2Type == null) || (input2TypeArgumentIndex >= 0), \"Input 2 type argument index was not provided\");\n    Preconditions.checkArgument(outputTypeArgumentIndex >= 0, \"Output type argument index was not provided\");\n    Preconditions.checkArgument(lambdaOutputTypeArgumentIndices != null, \"Indices for output type arguments within lambda not provided\");\n    // explicit result type has highest precedence\n    if (function instanceof ResultTypeQueryable) {\n        return ((ResultTypeQueryable<OUT>) (function)).getProducedType();\n    }\n    // perform extraction\n    try {\n        final LambdaExecutable exec;\n        try {\n            exec = checkAndExtractLambda(function);\n        } catch (TypeExtractionException e) {\n            throw new InvalidTypesException(\"Internal error occurred.\", e);\n        }\n        if (exec != null) {\n            final Method sam = TypeExtractionUtils.getSingleAbstractMethod(baseClass);\n            final int baseParametersLen = sam.getParameterCount();\n            // parameters must be accessed from behind, since JVM can add additional parameters\n            // e.g. when using local variables inside lambda function\n            final int paramLen = exec.getParameterTypes().length;\n            final Type output;\n            if (lambdaOutputTypeArgumentIndices.length > 0) {\n                output = TypeExtractionUtils.extractTypeFromLambda(baseClass, exec, lambdaOutputTypeArgumentIndices, paramLen, baseParametersLen);\n            } else {\n                output = exec.getReturnType();\n                TypeExtractionUtils.validateLambdaType(baseClass, output);\n            }\n            return new TypeExtractor().privateCreateTypeInfo(output, in1Type, in2Type);\n        } else {\n            if (in1Type != null) {\n                validateInputType(baseClass, function.getClass(), input1TypeArgumentIndex, in1Type);\n            }\n            if (in2Type != null) {\n                validateInputType(baseClass, function.getClass(), input2TypeArgumentIndex, in2Type);\n            }\n            return new TypeExtractor().privateCreateTypeInfo(baseClass, function.getClass(), outputTypeArgumentIndex, in1Type, in2Type);\n        }\n    } catch (InvalidTypesException e) {\n        if (allowMissing) {\n            return ((TypeInformation<OUT>) (new MissingTypeInfo(functionName != null ? functionName : function.toString(), e)));\n        } else {\n            throw e;\n        }\n    }\n}", "// for LambdaFunctions\nprivate <IN1, IN2, OUT> TypeInformation<OUT> privateCreateTypeInfo(Type returnType, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    List<Type> typeHierarchy = new ArrayList<>();\n    // get info from hierarchy\n    return createTypeInfoWithTypeHierarchy(typeHierarchy, returnType, in1Type, in2Type);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <IN1, IN2, OUT> TypeInformation<OUT> createTypeInfoWithTypeHierarchy(List<Type> typeHierarchy, Type t, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    // check if type information can be created using a type factory\n    final TypeInformation<OUT> typeFromFactory = createTypeInfoFromFactory(t, typeHierarchy, in1Type, in2Type);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    } else if (isClassType(t) && Tuple.class.isAssignableFrom(typeToClass(t))) {\n        Type curT = t;\n        // do not allow usage of Tuple as type\n        if (typeToClass(t).equals(Tuple.class)) {\n            throw new InvalidTypesException(\"Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead.\");\n        }\n        // go up the hierarchy until we reach immediate child of Tuple (with or without\n        // generics)\n        // collect the types while moving up for a later top-down\n        List<Type> typeHierarchyForSubtypes = new ArrayList<>(typeHierarchy);\n        while (!(isClassType(curT) && typeToClass(curT).getSuperclass().equals(Tuple.class))) {\n            typeHierarchyForSubtypes.add(curT);\n            curT = typeToClass(curT).getGenericSuperclass();\n        } \n        if (curT == Tuple0.class) {\n            return new TupleTypeInfo(Tuple0.class);\n        }\n        // check if immediate child of Tuple has generics\n        if (curT instanceof Class<?>) {\n            throw new InvalidTypesException(\"Tuple needs to be parameterized by using generics.\");\n        }\n        typeHierarchyForSubtypes.add(curT);\n        // create the type information for the subtypes\n        final TypeInformation<?>[] subTypesInfo = createSubTypesInfo(t, ((ParameterizedType) (curT)), typeHierarchyForSubtypes, in1Type, in2Type, false);\n        // type needs to be treated a pojo due to additional fields\n        if (subTypesInfo == null) {\n            return analyzePojo(t, new ArrayList<>(typeHierarchy), in1Type, in2Type);\n        }\n        for (int i = 0; i < subTypesInfo.length; i++) {\n            if (subTypesInfo[i] instanceof GenericTypeInfo) {\n                LOG.info(\"Tuple field #{} of type '{}' will be processed as GenericType. {}\", i + 1, subTypesInfo[i].getTypeClass().getSimpleName(), GENERIC_TYPE_DOC_HINT);\n            }\n        }\n        // return tuple info\n        return new TupleTypeInfo(typeToClass(t), subTypesInfo);\n    } else if (t instanceof TypeVariable) {\n        Type typeVar = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (t)));\n        if (!(typeVar instanceof TypeVariable)) {\n            return createTypeInfoWithTypeHierarchy(typeHierarchy, typeVar, in1Type, in2Type);\n        } else // try to derive the type info of the TypeVariable from the immediate base child input\n        // as a last attempt\n        {\n            TypeInformation<OUT> typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (t)), typeHierarchy, in1Type, in2Type)));\n            if (typeInfo != null) {\n                return typeInfo;\n            } else {\n                throw new InvalidTypesException(((((((\"Type of TypeVariable '\" + ((TypeVariable<?>) (t)).getName()) + \"' in '\") + ((TypeVariable<?>) (t)).getGenericDeclaration()) + \"' could not be determined. This is most likely a type erasure problem. \") + \"The type extraction currently supports types with generic variables only in cases where \") + \"all variables in the return type can be deduced from the input type(s). \") + \"Otherwise the type has to be specified explicitly using type information.\");\n            }\n        }\n    } else if (t instanceof GenericArrayType) {\n        GenericArrayType genericArray = ((GenericArrayType) (t));\n        Type componentType = genericArray.getGenericComponentType();\n        // due to a Java 6 bug, it is possible that the JVM classifies e.g. String[] or int[] as\n        // GenericArrayType instead of Class\n        if (componentType instanceof Class) {\n            Class<?> componentClass = ((Class<?>) (componentType));\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return getForClass(classArray);\n        } else {\n            TypeInformation<?> componentInfo = createTypeInfoWithTypeHierarchy(typeHierarchy, genericArray.getGenericComponentType(), in1Type, in2Type);\n            Class<?> componentClass = componentInfo.getTypeClass();\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return ObjectArrayTypeInfo.getInfoFor(classArray, componentInfo);\n        }\n    } else if (t instanceof ParameterizedType) {\n        return privateGetForClass(typeToClass(t), typeHierarchy, ((ParameterizedType) (t)), in1Type, in2Type);\n    } else if (t instanceof Class) {\n        return privateGetForClass(((Class<OUT>) (t)), typeHierarchy);\n    }\n    throw new InvalidTypesException(\"Type Information could not be created.\");\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType", "org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Generic extraction methods\n// --------------------------------------------------------------------------------------------\n/**\n * Returns the unary operator's return type.\n *\n * <p>This method can extract a type in 4 different ways:\n *\n * <p>1. By using the generics of the base class like MyFunction<X, Y, Z, IN, OUT>. This is what\n * outputTypeArgumentIndex (in this example \"4\") is good for.\n *\n * <p>2. By using input type inference SubMyFunction<T, String, String, String, T>. This is what\n * inputTypeArgumentIndex (in this example \"0\") and inType is good for.\n *\n * <p>3. By using the static method that a compiler generates for Java lambdas. This is what\n * lambdaOutputTypeArgumentIndices is good for. Given that MyFunction has the following single\n * abstract method:\n *\n * <pre>\n * <code>\n * void apply(IN value, Collector<OUT> value)\n * </code>\n * </pre>\n *\n * <p>Lambda type indices allow the extraction of a type from lambdas. To extract the output\n * type <b>OUT</b> from the function one should pass {@code new int[] {1, 0}}. \"1\" for selecting\n * the parameter and 0 for the first generic in this type. Use {@code TypeExtractor.NO_INDEX}\n * for selecting the return type of the lambda for extraction or if the class cannot be a lambda\n * because it is not a single abstract method interface.\n *\n * <p>4. By using interfaces such as {@link TypeInfoFactory} or {@link ResultTypeQueryable}.\n *\n * <p>See also comments in the header of this class.\n *\n * @param function\n * \t\tFunction to extract the return type from\n * @param baseClass\n * \t\tBase class of the function\n * @param inputTypeArgumentIndex\n * \t\tIndex of input generic type in the base class specification\n * \t\t(ignored if inType is null)\n * @param outputTypeArgumentIndex\n * \t\tIndex of output generic type in the base class specification\n * @param lambdaOutputTypeArgumentIndices\n * \t\tTable of indices of the type argument specifying the\n * \t\tinput type. See example.\n * @param inType\n * \t\tType of the input elements (In case of an iterable, it is the element type) or\n * \t\tnull\n * @param functionName\n * \t\tFunction name\n * @param allowMissing\n * \t\tCan the type information be missing (this generates a MissingTypeInfo for\n * \t\tpostponing an exception)\n * @param <IN>\n * \t\tInput type\n * @param <OUT>\n * \t\tOutput type\n * @return TypeInformation of the return type of the function\n */\n@SuppressWarnings(\"unchecked\")\n@PublicEvolving\npublic static <IN, OUT> TypeInformation<OUT> getUnaryOperatorReturnType(Function function, Class<?> baseClass, int inputTypeArgumentIndex, int outputTypeArgumentIndex, int[] lambdaOutputTypeArgumentIndices, TypeInformation<IN> inType, String functionName, boolean allowMissing) {\n    Preconditions.checkArgument((inType == null) || (inputTypeArgumentIndex >= 0), \"Input type argument index was not provided\");\n    Preconditions.checkArgument(outputTypeArgumentIndex >= 0, \"Output type argument index was not provided\");\n    Preconditions.checkArgument(lambdaOutputTypeArgumentIndices != null, \"Indices for output type arguments within lambda not provided\");\n    // explicit result type has highest precedence\n    if (function instanceof ResultTypeQueryable) {\n        return ((ResultTypeQueryable<OUT>) (function)).getProducedType();\n    }\n    // perform extraction\n    try {\n        final LambdaExecutable exec;\n        try {\n            exec = checkAndExtractLambda(function);\n        } catch (TypeExtractionException e) {\n            throw new InvalidTypesException(\"Internal error occurred.\", e);\n        }\n        if (exec != null) {\n            // parameters must be accessed from behind, since JVM can add additional parameters\n            // e.g. when using local variables inside lambda function\n            // paramLen is the total number of parameters of the provided lambda, it includes\n            // parameters added through closure\n            final int paramLen = exec.getParameterTypes().length;\n            final Method sam = TypeExtractionUtils.getSingleAbstractMethod(baseClass);\n            // number of parameters the SAM of implemented interface has; the parameter indexing\n            // applies to this range\n            final int baseParametersLen = sam.getParameterCount();\n            final Type output;\n            if (lambdaOutputTypeArgumentIndices.length > 0) {\n                output = TypeExtractionUtils.extractTypeFromLambda(baseClass, exec, lambdaOutputTypeArgumentIndices, paramLen, baseParametersLen);\n            } else {\n                output = exec.getReturnType();\n                TypeExtractionUtils.validateLambdaType(baseClass, output);\n            }\n            return new TypeExtractor().privateCreateTypeInfo(output, inType, null);\n        } else {\n            if (inType != null) {\n                validateInputType(baseClass, function.getClass(), inputTypeArgumentIndex, inType);\n            }\n            return new TypeExtractor().privateCreateTypeInfo(baseClass, function.getClass(), outputTypeArgumentIndex, inType, null);\n        }\n    } catch (InvalidTypesException e) {\n        if (allowMissing) {\n            return ((TypeInformation<OUT>) (new MissingTypeInfo(functionName != null ? functionName : function.toString(), e)));\n        } else {\n            throw e;\n        }\n    }\n}", "// for LambdaFunctions\nprivate <IN1, IN2, OUT> TypeInformation<OUT> privateCreateTypeInfo(Type returnType, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    List<Type> typeHierarchy = new ArrayList<>();\n    // get info from hierarchy\n    return createTypeInfoWithTypeHierarchy(typeHierarchy, returnType, in1Type, in2Type);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <IN1, IN2, OUT> TypeInformation<OUT> createTypeInfoWithTypeHierarchy(List<Type> typeHierarchy, Type t, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    // check if type information can be created using a type factory\n    final TypeInformation<OUT> typeFromFactory = createTypeInfoFromFactory(t, typeHierarchy, in1Type, in2Type);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    } else if (isClassType(t) && Tuple.class.isAssignableFrom(typeToClass(t))) {\n        Type curT = t;\n        // do not allow usage of Tuple as type\n        if (typeToClass(t).equals(Tuple.class)) {\n            throw new InvalidTypesException(\"Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead.\");\n        }\n        // go up the hierarchy until we reach immediate child of Tuple (with or without\n        // generics)\n        // collect the types while moving up for a later top-down\n        List<Type> typeHierarchyForSubtypes = new ArrayList<>(typeHierarchy);\n        while (!(isClassType(curT) && typeToClass(curT).getSuperclass().equals(Tuple.class))) {\n            typeHierarchyForSubtypes.add(curT);\n            curT = typeToClass(curT).getGenericSuperclass();\n        } \n        if (curT == Tuple0.class) {\n            return new TupleTypeInfo(Tuple0.class);\n        }\n        // check if immediate child of Tuple has generics\n        if (curT instanceof Class<?>) {\n            throw new InvalidTypesException(\"Tuple needs to be parameterized by using generics.\");\n        }\n        typeHierarchyForSubtypes.add(curT);\n        // create the type information for the subtypes\n        final TypeInformation<?>[] subTypesInfo = createSubTypesInfo(t, ((ParameterizedType) (curT)), typeHierarchyForSubtypes, in1Type, in2Type, false);\n        // type needs to be treated a pojo due to additional fields\n        if (subTypesInfo == null) {\n            return analyzePojo(t, new ArrayList<>(typeHierarchy), in1Type, in2Type);\n        }\n        for (int i = 0; i < subTypesInfo.length; i++) {\n            if (subTypesInfo[i] instanceof GenericTypeInfo) {\n                LOG.info(\"Tuple field #{} of type '{}' will be processed as GenericType. {}\", i + 1, subTypesInfo[i].getTypeClass().getSimpleName(), GENERIC_TYPE_DOC_HINT);\n            }\n        }\n        // return tuple info\n        return new TupleTypeInfo(typeToClass(t), subTypesInfo);\n    } else if (t instanceof TypeVariable) {\n        Type typeVar = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (t)));\n        if (!(typeVar instanceof TypeVariable)) {\n            return createTypeInfoWithTypeHierarchy(typeHierarchy, typeVar, in1Type, in2Type);\n        } else // try to derive the type info of the TypeVariable from the immediate base child input\n        // as a last attempt\n        {\n            TypeInformation<OUT> typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (t)), typeHierarchy, in1Type, in2Type)));\n            if (typeInfo != null) {\n                return typeInfo;\n            } else {\n                throw new InvalidTypesException(((((((\"Type of TypeVariable '\" + ((TypeVariable<?>) (t)).getName()) + \"' in '\") + ((TypeVariable<?>) (t)).getGenericDeclaration()) + \"' could not be determined. This is most likely a type erasure problem. \") + \"The type extraction currently supports types with generic variables only in cases where \") + \"all variables in the return type can be deduced from the input type(s). \") + \"Otherwise the type has to be specified explicitly using type information.\");\n            }\n        }\n    } else if (t instanceof GenericArrayType) {\n        GenericArrayType genericArray = ((GenericArrayType) (t));\n        Type componentType = genericArray.getGenericComponentType();\n        // due to a Java 6 bug, it is possible that the JVM classifies e.g. String[] or int[] as\n        // GenericArrayType instead of Class\n        if (componentType instanceof Class) {\n            Class<?> componentClass = ((Class<?>) (componentType));\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return getForClass(classArray);\n        } else {\n            TypeInformation<?> componentInfo = createTypeInfoWithTypeHierarchy(typeHierarchy, genericArray.getGenericComponentType(), in1Type, in2Type);\n            Class<?> componentClass = componentInfo.getTypeClass();\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return ObjectArrayTypeInfo.getInfoFor(classArray, componentInfo);\n        }\n    } else if (t instanceof ParameterizedType) {\n        return privateGetForClass(typeToClass(t), typeHierarchy, ((ParameterizedType) (t)), in1Type, in2Type);\n    } else if (t instanceof Class) {\n        return privateGetForClass(((Class<OUT>) (t)), typeHierarchy);\n    }\n    throw new InvalidTypesException(\"Type Information could not be created.\");\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.getForObject",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.getForObject", "org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForObject", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "public static <X> TypeInformation<X> getForObject(X value) {\n    return new TypeExtractor().privateGetForObject(value);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <X> TypeInformation<X> privateGetForObject(X value) {\n    checkNotNull(value);\n    // check if type information can be produced using a factory\n    final List<Type> typeHierarchy = new ArrayList<>();\n    typeHierarchy.add(value.getClass());\n    final TypeInformation<X> typeFromFactory = createTypeInfoFromFactory(value.getClass(), typeHierarchy, null, null);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    }\n    // check if we can extract the types from tuples, otherwise work with the class\n    if (value instanceof Tuple) {\n        Tuple t = ((Tuple) (value));\n        int numFields = t.getArity();\n        if (numFields != countFieldsInClass(value.getClass())) {\n            // not a tuple since it has more fields.\n            return analyzePojo(value.getClass(), new ArrayList<>(), null, null);// we immediately call analyze Pojo here, because\n\n            // there is currently no other type that can handle such a class.\n        }\n        TypeInformation<?>[] infos = new TypeInformation[numFields];\n        for (int i = 0; i < numFields; i++) {\n            Object field = t.getField(i);\n            if (field == null) {\n                throw new InvalidTypesException(\"Automatic type extraction is not possible on candidates with null values. \" + \"Please specify the types directly.\");\n            }\n            infos[i] = privateGetForObject(field);\n        }\n        return new TupleTypeInfo(value.getClass(), infos);\n    } else if (value instanceof Row) {\n        Row row = ((Row) (value));\n        int arity = row.getArity();\n        for (int i = 0; i < arity; i++) {\n            if (row.getField(i) == null) {\n                LOG.warn(((\"Cannot extract type of Row field, because of Row field[\" + i) + \"] is null. \") + \"Should define RowTypeInfo explicitly.\");\n                return privateGetForClass(((Class<X>) (value.getClass())), new ArrayList<>());\n            }\n        }\n        TypeInformation<?>[] typeArray = new TypeInformation<?>[arity];\n        for (int i = 0; i < arity; i++) {\n            typeArray[i] = TypeExtractor.getForObject(row.getField(i));\n        }\n        return ((TypeInformation<X>) (new RowTypeInfo(typeArray)));\n    } else {\n        return privateGetForClass(((Class<X>) (value.getClass())), new ArrayList<>());\n    }\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.getForClass",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.getForClass", "org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass", "org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "/**\n * Creates type information from a given Class such as Integer, String[] or POJOs.\n *\n * <p>This method does not support ParameterizedTypes such as Tuples or complex type\n * hierarchies. In most cases {@link TypeExtractor#createTypeInfo(Type)} is the recommended\n * method for type extraction (a Class is a child of Type).\n *\n * @param clazz\n * \t\ta Class to create TypeInformation for\n * @return TypeInformation that describes the passed Class\n */\npublic static <X> TypeInformation<X> getForClass(Class<X> clazz) {\n    final List<Type> typeHierarchy = new ArrayList<>();\n    typeHierarchy.add(clazz);\n    return new TypeExtractor().privateGetForClass(clazz, typeHierarchy);\n}", "private <X> TypeInformation<X> privateGetForClass(Class<X> clazz, List<Type> typeHierarchy) {\n    return privateGetForClass(clazz, typeHierarchy, null, null, null);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <OUT, IN1, IN2> TypeInformation<OUT> privateGetForClass(Class<OUT> clazz, List<Type> typeHierarchy, ParameterizedType parameterizedType, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    checkNotNull(clazz);\n    // check if type information can be produced using a factory\n    final TypeInformation<OUT> typeFromFactory = createTypeInfoFromFactory(clazz, typeHierarchy, in1Type, in2Type);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    }\n    // Object is handled as generic type info\n    if (clazz.equals(Object.class)) {\n        return new GenericTypeInfo<>(clazz);\n    }\n    // Class is handled as generic type info\n    if (clazz.equals(Class.class)) {\n        return new GenericTypeInfo<>(clazz);\n    }\n    // recursive types are handled as generic type info\n    if (countTypeInHierarchy(typeHierarchy, clazz) > 1) {\n        return new GenericTypeInfo<>(clazz);\n    }\n    // check for arrays\n    if (clazz.isArray()) {\n        // primitive arrays: int[], byte[], ...\n        PrimitiveArrayTypeInfo<OUT> primitiveArrayInfo = PrimitiveArrayTypeInfo.getInfoFor(clazz);\n        if (primitiveArrayInfo != null) {\n            return primitiveArrayInfo;\n        }\n        // basic type arrays: String[], Integer[], Double[]\n        BasicArrayTypeInfo<OUT, ?> basicArrayInfo = BasicArrayTypeInfo.getInfoFor(clazz);\n        if (basicArrayInfo != null) {\n            return basicArrayInfo;\n        } else // object arrays\n        {\n            TypeInformation<?> componentTypeInfo = createTypeInfoWithTypeHierarchy(typeHierarchy, clazz.getComponentType(), in1Type, in2Type);\n            return ObjectArrayTypeInfo.getInfoFor(clazz, componentTypeInfo);\n        }\n    }\n    // check for writable types\n    if (isHadoopWritable(clazz)) {\n        return createHadoopWritableTypeInfo(clazz);\n    }\n    // check for basic types\n    TypeInformation<OUT> basicTypeInfo = BasicTypeInfo.getInfoFor(clazz);\n    if (basicTypeInfo != null) {\n        return basicTypeInfo;\n    }\n    // check for SQL time types\n    TypeInformation<OUT> timeTypeInfo = SqlTimeTypeInfo.getInfoFor(clazz);\n    if (timeTypeInfo != null) {\n        return timeTypeInfo;\n    }\n    // check for subclasses of Value\n    if (Value.class.isAssignableFrom(clazz)) {\n        Class<? extends Value> valueClass = clazz.asSubclass(Value.class);\n        return ((TypeInformation<OUT>) (ValueTypeInfo.getValueTypeInfo(valueClass)));\n    }\n    // check for subclasses of Tuple\n    if (Tuple.class.isAssignableFrom(clazz)) {\n        if (clazz == Tuple0.class) {\n            return new TupleTypeInfo(Tuple0.class);\n        }\n        throw new InvalidTypesException(\"Type information extraction for tuples (except Tuple0) cannot be done based on the class.\");\n    }\n    // check for Enums\n    if (Enum.class.isAssignableFrom(clazz)) {\n        return new EnumTypeInfo(clazz);\n    }\n    // check for Variant\n    if (Variant.class.isAssignableFrom(clazz)) {\n        return ((TypeInformation<OUT>) (VariantTypeInfo.INSTANCE));\n    }\n    // check for parameterized Collections, requirement:\n    // 1. Interface types: the underlying implementation types are not preserved across\n    // serialization\n    // 2. Concrete type arguments: Flink needs them to dispatch serialization of element types\n    // Example:\n    // - OK: List<String>, Collection<String>\n    // - not OK: LinkedList<String> (implementation type), List (raw type), List<T> (generic\n    // type argument), or List<?> (wildcard type argument)\n    if (parameterizedType != null) {\n        Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();\n        boolean allTypeArgumentsConcrete = Arrays.stream(actualTypeArguments).allMatch(arg -> arg instanceof Class<?>);\n        if (allTypeArgumentsConcrete) {\n            if (clazz.isAssignableFrom(Map.class)) {\n                Class<?> keyClass = ((Class<?>) (actualTypeArguments[0]));\n                Class<?> valueClass = ((Class<?>) (actualTypeArguments[1]));\n                TypeInformation<?> keyTypeInfo = createTypeInfo(keyClass);\n                TypeInformation<?> valueTypeInfo = createTypeInfo(valueClass);\n                return ((TypeInformation<OUT>) (new NullableMapTypeInfo<>(keyTypeInfo, valueTypeInfo)));\n            } else if (clazz.isAssignableFrom(List.class)) {\n                Class<?> elementClass = ((Class<?>) (actualTypeArguments[0]));\n                TypeInformation<?> elementTypeInfo = createTypeInfo(elementClass);\n                return ((TypeInformation<OUT>) (new NullableListTypeInfo<>(elementTypeInfo)));\n            } else if (clazz.isAssignableFrom(Set.class)) {\n                Class<?> elementClass = ((Class<?>) (actualTypeArguments[0]));\n                TypeInformation<?> elementTypeInfo = createTypeInfo(elementClass);\n                return ((TypeInformation<OUT>) (new NullableSetTypeInfo<>(elementTypeInfo)));\n            }\n        }\n    }\n    // special case for POJOs generated by Avro.\n    if (AvroUtils.isAvroSpecificRecord(clazz)) {\n        return AvroUtils.getAvroUtils().createAvroTypeInfo(clazz);\n    }\n    if (Modifier.isInterface(clazz.getModifiers())) {\n        // Interface has no members and is therefore not handled as POJO\n        return new GenericTypeInfo<>(clazz);\n    }\n    try {\n        Type t = (parameterizedType != null) ? parameterizedType : clazz;\n        TypeInformation<OUT> pojoType = analyzePojo(t, new ArrayList<>(typeHierarchy), in1Type, in2Type);\n        if (pojoType != null) {\n            return pojoType;\n        }\n    } catch (InvalidTypesException e) {\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(((\"Unable to handle type \" + clazz) + \" as POJO. Message: \") + e.getMessage(), e);\n        }\n        // ignore and create generic type info\n    }\n    // return a generic type\n    return new GenericTypeInfo<>(clazz);\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.primitiveToWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo", "org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy", "org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo", "org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField" ],
    "fullMethods" : [ "@PublicEvolving\npublic static <IN1, IN2, OUT> TypeInformation<OUT> createTypeInfo(Class<?> baseClass, Class<?> clazz, int returnParamPos, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    TypeInformation<OUT> ti = new TypeExtractor().privateCreateTypeInfo(baseClass, clazz, returnParamPos, in1Type, in2Type);\n    if (ti == null) {\n        throw new InvalidTypesException(\"Could not extract type information.\");\n    }\n    return ti;\n}", "// for (Rich)Functions\n@SuppressWarnings(\"unchecked\")\nprivate <IN1, IN2, OUT> TypeInformation<OUT> privateCreateTypeInfo(Class<?> baseClass, Class<?> clazz, int returnParamPos, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    List<Type> typeHierarchy = new ArrayList<>();\n    Type returnType = getParameterType(baseClass, typeHierarchy, clazz, returnParamPos);\n    TypeInformation<OUT> typeInfo;\n    // return type is a variable -> try to get the type info from the input directly\n    if (returnType instanceof TypeVariable<?>) {\n        typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (returnType)), typeHierarchy, in1Type, in2Type)));\n        if (typeInfo != null) {\n            return typeInfo;\n        }\n    }\n    // get info from hierarchy\n    return createTypeInfoWithTypeHierarchy(typeHierarchy, returnType, in1Type, in2Type);\n}", "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\nprivate <IN1, IN2, OUT> TypeInformation<OUT> createTypeInfoWithTypeHierarchy(List<Type> typeHierarchy, Type t, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    // check if type information can be created using a type factory\n    final TypeInformation<OUT> typeFromFactory = createTypeInfoFromFactory(t, typeHierarchy, in1Type, in2Type);\n    if (typeFromFactory != null) {\n        return typeFromFactory;\n    } else if (isClassType(t) && Tuple.class.isAssignableFrom(typeToClass(t))) {\n        Type curT = t;\n        // do not allow usage of Tuple as type\n        if (typeToClass(t).equals(Tuple.class)) {\n            throw new InvalidTypesException(\"Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead.\");\n        }\n        // go up the hierarchy until we reach immediate child of Tuple (with or without\n        // generics)\n        // collect the types while moving up for a later top-down\n        List<Type> typeHierarchyForSubtypes = new ArrayList<>(typeHierarchy);\n        while (!(isClassType(curT) && typeToClass(curT).getSuperclass().equals(Tuple.class))) {\n            typeHierarchyForSubtypes.add(curT);\n            curT = typeToClass(curT).getGenericSuperclass();\n        } \n        if (curT == Tuple0.class) {\n            return new TupleTypeInfo(Tuple0.class);\n        }\n        // check if immediate child of Tuple has generics\n        if (curT instanceof Class<?>) {\n            throw new InvalidTypesException(\"Tuple needs to be parameterized by using generics.\");\n        }\n        typeHierarchyForSubtypes.add(curT);\n        // create the type information for the subtypes\n        final TypeInformation<?>[] subTypesInfo = createSubTypesInfo(t, ((ParameterizedType) (curT)), typeHierarchyForSubtypes, in1Type, in2Type, false);\n        // type needs to be treated a pojo due to additional fields\n        if (subTypesInfo == null) {\n            return analyzePojo(t, new ArrayList<>(typeHierarchy), in1Type, in2Type);\n        }\n        for (int i = 0; i < subTypesInfo.length; i++) {\n            if (subTypesInfo[i] instanceof GenericTypeInfo) {\n                LOG.info(\"Tuple field #{} of type '{}' will be processed as GenericType. {}\", i + 1, subTypesInfo[i].getTypeClass().getSimpleName(), GENERIC_TYPE_DOC_HINT);\n            }\n        }\n        // return tuple info\n        return new TupleTypeInfo(typeToClass(t), subTypesInfo);\n    } else if (t instanceof TypeVariable) {\n        Type typeVar = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (t)));\n        if (!(typeVar instanceof TypeVariable)) {\n            return createTypeInfoWithTypeHierarchy(typeHierarchy, typeVar, in1Type, in2Type);\n        } else // try to derive the type info of the TypeVariable from the immediate base child input\n        // as a last attempt\n        {\n            TypeInformation<OUT> typeInfo = ((TypeInformation<OUT>) (createTypeInfoFromInputs(((TypeVariable<?>) (t)), typeHierarchy, in1Type, in2Type)));\n            if (typeInfo != null) {\n                return typeInfo;\n            } else {\n                throw new InvalidTypesException(((((((\"Type of TypeVariable '\" + ((TypeVariable<?>) (t)).getName()) + \"' in '\") + ((TypeVariable<?>) (t)).getGenericDeclaration()) + \"' could not be determined. This is most likely a type erasure problem. \") + \"The type extraction currently supports types with generic variables only in cases where \") + \"all variables in the return type can be deduced from the input type(s). \") + \"Otherwise the type has to be specified explicitly using type information.\");\n            }\n        }\n    } else if (t instanceof GenericArrayType) {\n        GenericArrayType genericArray = ((GenericArrayType) (t));\n        Type componentType = genericArray.getGenericComponentType();\n        // due to a Java 6 bug, it is possible that the JVM classifies e.g. String[] or int[] as\n        // GenericArrayType instead of Class\n        if (componentType instanceof Class) {\n            Class<?> componentClass = ((Class<?>) (componentType));\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return getForClass(classArray);\n        } else {\n            TypeInformation<?> componentInfo = createTypeInfoWithTypeHierarchy(typeHierarchy, genericArray.getGenericComponentType(), in1Type, in2Type);\n            Class<?> componentClass = componentInfo.getTypeClass();\n            Class<OUT> classArray = ((Class<OUT>) (Array.newInstance(componentClass, 0).getClass()));\n            return ObjectArrayTypeInfo.getInfoFor(classArray, componentInfo);\n        }\n    } else if (t instanceof ParameterizedType) {\n        return privateGetForClass(typeToClass(t), typeHierarchy, ((ParameterizedType) (t)), in1Type, in2Type);\n    } else if (t instanceof Class) {\n        return privateGetForClass(((Class<OUT>) (t)), typeHierarchy);\n    }\n    throw new InvalidTypesException(\"Type Information could not be created.\");\n}", "@SuppressWarnings(\"unchecked\")\nprotected <OUT, IN1, IN2> TypeInformation<OUT> analyzePojo(Type type, List<Type> typeHierarchy, TypeInformation<IN1> in1Type, TypeInformation<IN2> in2Type) {\n    Class<OUT> clazz = typeToClass(type);\n    if (!Modifier.isPublic(clazz.getModifiers())) {\n        LOG.info(((\"Class \" + clazz.getName()) + \" is not public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    // add the hierarchy of the POJO\n    getTypeHierarchy(typeHierarchy, type, Object.class);\n    List<Field> fields = getAllDeclaredFields(clazz, false);\n    if (fields.size() == 0) {\n        LOG.info(((\"No fields were detected for \" + clazz) + \" so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return new GenericTypeInfo<>(clazz);\n    }\n    boolean isRecord = isRecord(clazz);\n    List<PojoField> pojoFields = new ArrayList<>();\n    for (Field field : fields) {\n        Type fieldType = field.getGenericType();\n        if (((!isRecord) && (!isValidPojoField(field, clazz, typeHierarchy))) && (clazz != Row.class)) {\n            LOG.info(((\"Class \" + clazz) + \" cannot be used as a POJO type because not all fields are valid POJO fields, \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n        try {\n            final TypeInformation<?> typeInfo;\n            List<Type> fieldTypeHierarchy = new ArrayList<>(typeHierarchy);\n            TypeInfoFactory factory = getTypeInfoFactory(field);\n            if (factory != null) {\n                typeInfo = createTypeInfoFromFactory(fieldType, in1Type, in2Type, fieldTypeHierarchy, factory, fieldType);\n            } else {\n                fieldTypeHierarchy.add(fieldType);\n                typeInfo = createTypeInfoWithTypeHierarchy(fieldTypeHierarchy, fieldType, in1Type, in2Type);\n            }\n            if (typeInfo instanceof GenericTypeInfo) {\n                LOG.info(\"Field {}#{} will be processed as GenericType. {}\", clazz.getSimpleName(), field.getName(), GENERIC_TYPE_DOC_HINT);\n            }\n            pojoFields.add(new PojoField(field, typeInfo));\n        } catch (InvalidTypesException e) {\n            Class<?> genericClass = Object.class;\n            if (isClassType(fieldType)) {\n                genericClass = typeToClass(fieldType);\n            }\n            pojoFields.add(new PojoField(field, new GenericTypeInfo<>(((Class<OUT>) (genericClass)))));\n        }\n    }\n    CompositeType<OUT> pojoType = new PojoTypeInfo<>(clazz, pojoFields);\n    // \n    // Validate the correctness of the pojo.\n    // returning \"null\" will result create a generic type information.\n    // \n    List<Method> methods = getAllDeclaredMethods(clazz);\n    for (Method method : methods) {\n        if (method.getName().equals(\"readObject\") || method.getName().equals(\"writeObject\")) {\n            LOG.info(((\"Class \" + clazz) + \" contains custom serialization methods we do not call, so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if (isRecord) {\n        // no default constructor extraction needs to be applied for Java records\n        return pojoType;\n    }\n    // Try retrieving the default constructor, if it does not have one\n    // we cannot use this because the serializer uses it.\n    Constructor<OUT> defaultConstructor = null;\n    try {\n        defaultConstructor = clazz.getDeclaredConstructor();\n    } catch (NoSuchMethodException e) {\n        if (clazz.isInterface() || Modifier.isAbstract(clazz.getModifiers())) {\n            LOG.info((clazz + \" is abstract or an interface, having a concrete \") + \"type can increase performance.\");\n        } else {\n            LOG.info((clazz + \" is missing a default constructor so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n            return null;\n        }\n    }\n    if ((defaultConstructor != null) && (!Modifier.isPublic(defaultConstructor.getModifiers()))) {\n        LOG.info(((\"The default constructor of \" + clazz) + \" is not Public so it cannot be used as a POJO type \") + \"and must be processed as GenericType. {}\", GENERIC_TYPE_DOC_HINT);\n        return null;\n    }\n    // everything is checked, we return the pojo\n    return pojoType;\n}", "/**\n * Checks if the given field is a valid pojo field: - it is public OR - there are getter and\n * setter methods for the field.\n *\n * @param f\n * \t\tfield to check\n * @param clazz\n * \t\tclass of field\n * @param typeHierarchy\n * \t\ttype hierarchy for materializing generic types\n */\nprivate boolean isValidPojoField(Field f, Class<?> clazz, List<Type> typeHierarchy) {\n    if (Modifier.isPublic(f.getModifiers())) {\n        return true;\n    } else {\n        boolean hasGetter = false;\n        boolean hasSetter = false;\n        final String fieldNameLow = f.getName().toLowerCase().replaceAll(\"_\", \"\");\n        Type fieldType = f.getGenericType();\n        Class<?> fieldTypeWrapper = ClassUtils.primitiveToWrapper(f.getType());\n        TypeVariable<?> fieldTypeGeneric = null;\n        if (fieldType instanceof TypeVariable) {\n            fieldTypeGeneric = ((TypeVariable<?>) (fieldType));\n            fieldType = materializeTypeVariable(typeHierarchy, ((TypeVariable<?>) (fieldType)));\n        }\n        for (Method m : clazz.getMethods()) {\n            final String methodNameLow = (m.getName().endsWith(\"_$eq\")) ? m.getName().toLowerCase().replaceAll(\"_\", \"\").replaceFirst(\"\\\\$eq$\", \"_\\\\$eq\") : m.getName().toLowerCase().replaceAll(\"_\", \"\");\n            // check for getter\n            // The name should be \"get<FieldName>\" or \"<fieldName>\" (for scala) or\n            // \"is<fieldName>\" for boolean fields.\n            if ((((methodNameLow.equals(\"get\" + fieldNameLow) || methodNameLow.equals(\"is\" + fieldNameLow)) || methodNameLow.equals(fieldNameLow)) && // no arguments for the getter\n            (m.getParameterCount() == 0)) && // return type is same as field type (or the generic variant of it)\n            ((m.getGenericReturnType().equals(fieldType) || m.getReturnType().equals(fieldTypeWrapper)) || m.getGenericReturnType().equals(fieldTypeGeneric))) {\n                hasGetter = true;\n            }\n            // check for setters (<FieldName>_$eq for scala)\n            if (((((methodNameLow.equals(\"set\" + fieldNameLow) || methodNameLow.equals(fieldNameLow + \"_$eq\")) || (fieldNameLow.startsWith(\"is\") && methodNameLow.equals(\"set\" + fieldNameLow.substring(2)))) && (m.getParameterCount() == 1)) && // one parameter of the field's type\n            ((m.getGenericParameterTypes()[0].equals(fieldType) || m.getParameterTypes()[0].equals(fieldTypeWrapper)) || m.getGenericParameterTypes()[0].equals(fieldTypeGeneric))) && // return type is void (or the class self).\n            (m.getReturnType().equals(Void.TYPE) || m.getReturnType().equals(clazz))) {\n                hasSetter = true;\n            }\n        }\n        if (hasGetter && hasSetter) {\n            return true;\n        } else {\n            if ((!hasGetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a getter for field \") + f.getName());\n            }\n            if ((!hasSetter) && (clazz != Row.class)) {\n                LOG.info((clazz + \" does not contain a setter for field \") + f.getName());\n            }\n            return false;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate",
    "thirdPartyMethod" : "org.apache.commons.lang3.exception.CloneFailedException.<init>",
    "thirdPartyPackage" : "org.apache.commons.lang3.exception",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.duplicate", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.<init>", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deepCopySerializer" ],
    "fullMethods" : [ "@Override\npublic KryoSerializer<T> duplicate() {\n    return new KryoSerializer<>(this);\n}", "/**\n * Copy-constructor that does not copy transient fields. They will be initialized once required.\n */\nprotected KryoSerializer(KryoSerializer<T> toCopy) {\n    this.type = checkNotNull(toCopy.type, \"Type class cannot be null.\");\n    this.defaultSerializerClasses = toCopy.defaultSerializerClasses;\n    this.defaultSerializers = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.defaultSerializers.size());\n    this.kryoRegistrations = CollectionUtil.newLinkedHashMapWithExpectedSize(toCopy.kryoRegistrations.size());\n    // deep copy the serializer instances in defaultSerializers\n    for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : toCopy.defaultSerializers.entrySet()) {\n        this.defaultSerializers.put(entry.getKey(), deepCopySerializer(entry.getValue()));\n    }\n    // deep copy the serializer instances in kryoRegistrations\n    for (Map.Entry<String, KryoRegistration> entry : toCopy.kryoRegistrations.entrySet()) {\n        KryoRegistration kryoRegistration = entry.getValue();\n        if (kryoRegistration.getSerializerDefinitionType() == KryoRegistration.SerializerDefinitionType.INSTANCE) {\n            SerializableSerializer<? extends Serializer<?>> serializerInstance = kryoRegistration.getSerializableSerializerInstance();\n            if (serializerInstance != null) {\n                kryoRegistration = new KryoRegistration(kryoRegistration.getRegisteredClass(), deepCopySerializer(serializerInstance));\n            }\n        }\n        this.kryoRegistrations.put(entry.getKey(), kryoRegistration);\n    }\n}", "private SerializableSerializer<? extends Serializer<?>> deepCopySerializer(SerializableSerializer<? extends Serializer<?>> original) {\n    try {\n        return InstantiationUtil.clone(original, Thread.currentThread().getContextClassLoader());\n    } catch (IOException | ClassNotFoundException ex) {\n        throw new CloneFailedException(\"Could not clone serializer instance of class \" + original.getClass(), ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.create",
    "thirdPartyMethod" : "org.apache.commons.compress.compressors.zstandard.ZstdCompressorInputStream.<init>",
    "thirdPartyPackage" : "org.apache.commons.compress.compressors.zstandard",
    "path" : [ "org.apache.flink.api.common.io.compression.ZStandardInputStreamFactory.create" ],
    "fullMethods" : [ "@Override\npublic ZstdCompressorInputStream create(InputStream in) throws IOException {\n    return new ZstdCompressorInputStream(in);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.readClassAndObject",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.Load.loadFromInputStream",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.copy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy" ],
    "fullMethods" : [ "/**\n * Tries to copy the given record from using the provided Kryo instance. If this fails, then the\n * record from is copied by serializing it into a byte buffer and deserializing it from there.\n *\n * @param from\n * \t\tElement to copy\n * @param kryo\n * \t\tKryo instance to use\n * @param serializer\n * \t\tTypeSerializer which is used in case of a Kryo failure\n * @param <T>\n * \t\tType of the element to be copied\n * @return Copied element\n */\npublic static <T> T copy(T from, Kryo kryo, TypeSerializer<T> serializer) {\n    try {\n        return kryo.copy(from);\n    } catch (KryoException ke) {\n        // Kryo could not copy the object --> try to serialize/deserialize the object\n        try {\n            byte[] byteArray = InstantiationUtil.serializeToByteArray(serializer, from);\n            return InstantiationUtil.deserializeFromByteArray(serializer, byteArray);\n        } catch (IOException ioe) {\n            throw new RuntimeException(\"Could not copy object by serializing/deserializing\" + \" it.\", ioe);\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.copy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.KryoUtils.copy" ],
    "fullMethods" : [ "/**\n * Tries to copy the given record from using the provided Kryo instance. If this fails, then the\n * record from is copied by serializing it into a byte buffer and deserializing it from there.\n *\n * @param from\n * \t\tElement to copy\n * @param reuse\n * \t\tReuse element for the deserialization\n * @param kryo\n * \t\tKryo instance to use\n * @param serializer\n * \t\tTypeSerializer which is used in case of a Kryo failure\n * @param <T>\n * \t\tType of the element to be copied\n * @return Copied element\n */\npublic static <T> T copy(T from, T reuse, Kryo kryo, TypeSerializer<T> serializer) {\n    try {\n        return kryo.copy(from);\n    } catch (KryoException ke) {\n        // Kryo could not copy the object --> try to serialize/deserialize the object\n        try {\n            byte[] byteArray = InstantiationUtil.serializeToByteArray(serializer, from);\n            return InstantiationUtil.deserializeFromByteArray(serializer, reuse, byteArray);\n        } catch (IOException ioe) {\n            throw new RuntimeException(\"Could not copy object by serializing/deserializing\" + \" it.\", ioe);\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.copy",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.io.compression.Bzip2InputStreamFactory.create",
    "thirdPartyMethod" : "org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream.<init>",
    "thirdPartyPackage" : "org.apache.commons.compress.compressors.bzip2",
    "path" : [ "org.apache.flink.api.common.io.compression.Bzip2InputStreamFactory.create" ],
    "fullMethods" : [ "@Override\npublic BZip2CompressorInputStream create(InputStream in) throws IOException {\n    return new BZip2CompressorInputStream(in);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.ClosureCleaner.clean",
    "thirdPartyMethod" : "org.apache.commons.lang3.ClassUtils.isPrimitiveOrWrapper",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.api.java.ClosureCleaner.clean", "org.apache.flink.api.java.ClosureCleaner.clean" ],
    "fullMethods" : [ "/**\n * Tries to clean the closure of the given object, if the object is a non-static inner class.\n *\n * @param func\n * \t\tThe object whose closure should be cleaned.\n * @param level\n * \t\tthe clean up level.\n * @param checkSerializable\n * \t\tFlag to indicate whether serializability should be checked after the\n * \t\tclosure cleaning attempt.\n * @throws InvalidProgramException\n * \t\tThrown, if 'checkSerializable' is true, and the object was\n * \t\tnot serializable after the closure cleaning.\n * @throws RuntimeException\n * \t\tA RuntimeException may be thrown, if the code of the class could not\n * \t\tbe loaded, in order to process during the closure cleaning.\n */\npublic static void clean(Object func, ExecutionConfig.ClosureCleanerLevel level, boolean checkSerializable) {\n    clean(func, level, checkSerializable, Collections.newSetFromMap(new IdentityHashMap<>()));\n}", "private static void clean(Object func, ExecutionConfig.ClosureCleanerLevel level, boolean checkSerializable, Set<Object> visited) {\n    if (func == null) {\n        return;\n    }\n    if (!visited.add(func)) {\n        return;\n    }\n    final Class<?> cls = func.getClass();\n    if (ClassUtils.isPrimitiveOrWrapper(cls)) {\n        return;\n    }\n    if (usesCustomSerialization(cls)) {\n        return;\n    }\n    if (canBeSerialized(func)) {\n        return;\n    }\n    // serialization failed; try cleaning closure as a fallback\n    // First find the field name of the \"this$0\" field, this can\n    // be \"this$x\" depending on the nesting\n    boolean closureAccessed = false;\n    for (Field f : cls.getDeclaredFields()) {\n        if (f.getName().startsWith(\"this$\")) {\n            // found a closure referencing field - now try to clean\n            closureAccessed |= cleanThis0(func, cls, f.getName());\n        } else {\n            Object fieldObject;\n            try {\n                f.setAccessible(true);\n                fieldObject = f.get(func);\n            } catch (IllegalAccessException e) {\n                throw new RuntimeException(String.format(\"Can not access to the %s field in Class %s\", f.getName(), func.getClass()));\n            }\n            /* we should do a deep clean when we encounter an anonymous class, inner class and local class, but should\n            skip the class with custom serialize method.\n\n            There are five kinds of classes (or interfaces):\n            a) Top level classes\n            b) Nested classes (static member classes)\n            c) Inner classes (non-static member classes)\n            d) Local classes (named classes declared within a method)\n            e) Anonymous classes\n             */\n            if ((level == ExecutionConfig.ClosureCleanerLevel.RECURSIVE) && needsRecursion(f, fieldObject)) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Dig to clean the {}\", fieldObject.getClass().getName());\n                }\n                clean(fieldObject, ExecutionConfig.ClosureCleanerLevel.RECURSIVE, true, visited);\n            }\n        }\n    }\n    if (checkSerializable) {\n        try {\n            InstantiationUtil.serializeObject(func);\n        } catch (Exception e) {\n            String functionType = getSuperClassOrInterfaceName(func.getClass());\n            String msg = (functionType == null) ? func + \" is not serializable.\" : (\"The implementation of the \" + functionType) + \" is not serializable.\";\n            if (closureAccessed) {\n                msg += ((\" The implementation accesses fields of its enclosing class, which is \" + \"a common reason for non-serializability. \") + \"A common solution is to make the function a proper (non-inner) class, or \") + \"a static inner class.\";\n            } else {\n                msg += \" The object probably contains or references non serializable fields.\";\n            }\n            throw new InvalidProgramException(msg, e);\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveEntry.getDirectoryEntries",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava", "org.apache.flink.util.CompressionUtils.unpackEntry" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}", "private static void unpackEntry(TarArchiveInputStream tis, TarArchiveEntry entry, File targetDir) throws IOException {\n    String targetDirPath = targetDir.getCanonicalPath() + File.separator;\n    File outputFile = new File(targetDir, entry.getName());\n    if (!outputFile.getCanonicalPath().startsWith(targetDirPath)) {\n        throw new IOException(((\"expanding \" + entry.getName()) + \" would create entry outside of \") + targetDir);\n    }\n    if (entry.isDirectory()) {\n        if ((!outputFile.mkdirs()) && (!outputFile.isDirectory())) {\n            throw new IOException(\"Failed to create directory \" + outputFile);\n        }\n        for (TarArchiveEntry e : entry.getDirectoryEntries()) {\n            unpackEntry(tis, e, outputFile);\n        }\n        return;\n    }\n    if (entry.isSymbolicLink()) {\n        // create symbolic link relative to tar parent dir\n        Files.createSymbolicLink(Paths.get(new File(targetDir, entry.getName()).getCanonicalPath()), Paths.get(entry.getLinkName()));\n        return;\n    }\n    if (!outputFile.getParentFile().exists()) {\n        if (!outputFile.getParentFile().mkdirs()) {\n            throw new IOException(\"Mkdirs failed to create tar internal dir \" + targetDir);\n        }\n    }\n    try (OutputStream o = Files.newOutputStream(Paths.get(outputFile.getCanonicalPath()))) {\n        IOUtils.copyBytes(tis, o, false);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace",
    "thirdPartyMethod" : "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.setValue",
    "thirdPartyPackage" : "org.apache.commons.collections.map.AbstractHashedMap",
    "path" : [ "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && buildComparator.equalToReference(((BT) (entry.getValue())))) {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.read" ],
    "fullMethods" : [ "@Override\npublic int read(byte[] bytes, int offset, int count) throws KryoException {\n    if (bytes == null) {\n        throw new IllegalArgumentException(\"bytes cannot be null.\");\n    }\n    try {\n        return inputStream.read(bytes, offset, count);\n    } catch (IOException ex) {\n        throw new KryoException(ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes" ],
    "fullMethods" : [ "@Override\npublic void readBytes(byte[] bytes, int offset, int count) throws KryoException {\n    if (bytes == null) {\n        throw new IllegalArgumentException(\"bytes cannot be null.\");\n    }\n    if (count == 0) {\n        return;\n    }\n    try {\n        int bytesRead = 0;\n        int c;\n        while (true) {\n            c = inputStream.read(bytes, offset + bytesRead, count - bytesRead);\n            if (c == (-1)) {\n                throw new KryoException(new EOFException(\"No more bytes left.\"));\n            }\n            bytesRead += c;\n            if (bytesRead == count) {\n                break;\n            }\n        } \n    } catch (IOException ex) {\n        throw new KryoException(ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.skip",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.KryoException.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.skip" ],
    "fullMethods" : [ "@Override\npublic void skip(int count) throws KryoException {\n    try {\n        inputStream.skip(count);\n    } catch (IOException ex) {\n        throw new KryoException(ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.KryoUtils.applyRegistrations",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.register",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.KryoUtils.applyRegistrations" ],
    "fullMethods" : [ "/**\n * Apply a list of {@link KryoRegistration} to a Kryo instance. The list of registrations is\n * assumed to already be a final resolution of all possible registration overwrites.\n *\n * <p>The registrations are applied in the given order and always specify the registration id,\n * using the given {@code firstRegistrationId} and incrementing it for each registration.\n *\n * @param kryo\n * \t\tthe Kryo instance to apply the registrations\n * @param resolvedRegistrations\n * \t\tthe registrations, which should already be resolved of all\n * \t\tpossible registration overwrites\n * @param firstRegistrationId\n * \t\tthe first registration id to use\n */\npublic static void applyRegistrations(Kryo kryo, Collection<KryoRegistration> resolvedRegistrations, int firstRegistrationId) {\n    int currentRegistrationId = firstRegistrationId;\n    Serializer<?> serializer;\n    for (KryoRegistration registration : resolvedRegistrations) {\n        serializer = registration.getSerializer(kryo);\n        if (serializer != null) {\n            kryo.register(registration.getRegisteredClass(), serializer, currentRegistrationId);\n        } else {\n            kryo.register(registration.getRegisteredClass(), currentRegistrationId);\n        }\n        // if Kryo already had a serializer for that type then it ignores the registration\n        if (kryo.getRegistration(currentRegistrationId) != null) {\n            currentRegistrationId++;\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.getName",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.<init>",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.exceptions.YamlEngineException.<init>",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.exceptions",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.wrapExceptionToHiddenSensitiveData" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "/**\n * This method wraps a MarkedYAMLException to hide sensitive data in its message. Before using\n * this method, an exception message might include sensitive information like:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1:\n *     key1: secret1\n *     ^\n * found duplicate key key1\n * in 'reader', line 2, column 1:\n *     key1: secret2\n *     ^}</pre>\n *\n * <p>After using this method, the message will be sanitized to hide the sensitive details:\n *\n * <pre>{@code while constructing a mapping\n * in 'reader', line 1, column 1\n * found duplicate key key1\n * in 'reader', line 2, column 1}</pre>\n *\n * @param exception\n * \t\tThe MarkedYamlEngineException containing potentially sensitive data.\n * @return A YamlEngineException with a message that has sensitive data hidden.\n */\nprivate static YamlEngineException wrapExceptionToHiddenSensitiveData(MarkedYamlEngineException exception) {\n    StringBuilder lines = new StringBuilder();\n    String context = exception.getContext();\n    Optional<Mark> contextMark = exception.getContextMark();\n    Optional<Mark> problemMark = exception.getProblemMark();\n    String problem = exception.getProblem();\n    if (context != null) {\n        lines.append(context);\n        lines.append(\"\\n\");\n    }\n    if (contextMark.isPresent() && (((((problem == null) || (!problemMark.isPresent())) || contextMark.get().getName().equals(problemMark.get().getName())) || (contextMark.get().getLine() != problemMark.get().getLine())) || (contextMark.get().getColumn() != problemMark.get().getColumn()))) {\n        lines.append(hiddenSensitiveDataInMark(contextMark.get()));\n        lines.append(\"\\n\");\n    }\n    if (problem != null) {\n        lines.append(problem);\n        lines.append(\"\\n\");\n    }\n    if (problemMark.isPresent()) {\n        lines.append(hiddenSensitiveDataInMark(problemMark.get()));\n        lines.append(\"\\n\");\n    }\n    Throwable cause = exception.getCause();\n    if (cause instanceof MarkedYamlEngineException) {\n        cause = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (cause)));\n    }\n    YamlEngineException yamlException = new YamlEngineException(lines.toString(), cause);\n    yamlException.setStackTrace(exception.getStackTrace());\n    return yamlException;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T copy(T from) {\n    checkKryoInitialized();\n    return KryoUtils.copy(from, kryo, this);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.ValueSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (this.copyInstance == null) {\n        this.copyInstance = InstantiationUtil.instantiate(type);\n    }\n    this.copyInstance.read(source);\n    this.copyInstance.write(target);\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(initStrategy);\n        // this.kryo.setAsmEnabled(true);\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), this.kryo.getNextRegistrationId());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}", "// --------------------------------------------------------------------------------------------\n/**\n * Returns the Chill Kryo Serializer which is implicitly added to the classpath via\n * flink-runtime. Falls back to the default Kryo serializer if it can't be found.\n *\n * @return The Kryo serializer instance.\n */\nprivate Kryo getKryoInstance() {\n    try {\n        // check if ScalaKryoInstantiator is in class path (coming from Twitter's Chill\n        // library).\n        // This will be true if Flink's Table Api Scala is used.\n        Class<?> chillInstantiatorClazz = Class.forName(\"org.apache.flink.table.api.runtime.types.FlinkScalaKryoInstantiator\");\n        Object chillInstantiator = chillInstantiatorClazz.newInstance();\n        // obtain a Kryo instance through Twitter Chill\n        Method m = chillInstantiatorClazz.getMethod(\"newKryo\");\n        return ((Kryo) (m.invoke(chillInstantiator)));\n    } catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) {\n        Optional<Kryo> kryoInstanceFromLegacyPackage = getKryoInstanceFromLegacyPackage();\n        if (kryoInstanceFromLegacyPackage.isPresent()) {\n            return kryoInstanceFromLegacyPackage.get();\n        }\n        if (LOG.isDebugEnabled()) {\n            LOG.info(\"Kryo serializer scala extensions are not available.\", e);\n        } else {\n            LOG.info(\"Kryo serializer scala extensions are not available.\");\n        }\n        DefaultInstantiatorStrategy initStrategy = new DefaultInstantiatorStrategy();\n        initStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        Kryo kryo = new Kryo();\n        kryo.setInstantiatorStrategy(initStrategy);\n        if (flinkChillPackageRegistrar != null) {\n            flinkChillPackageRegistrar.registerSerializers(kryo);\n        }\n        return kryo;\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.<init>",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference", "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.register",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.ValueComparator.setReference", "org.apache.flink.api.java.typeutils.runtime.ValueComparator.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void setReference(T toCompare) {\n    checkKryoInitialized();\n    reference = KryoUtils.copy(toCompare, kryo, new ValueSerializer<T>(type));\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = new Kryo();\n        DefaultInstantiatorStrategy instantiatorStrategy = new DefaultInstantiatorStrategy();\n        instantiatorStrategy.setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());\n        kryo.setInstantiatorStrategy(instantiatorStrategy);\n        this.kryo.register(type);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.GlobalConfiguration.loadConfiguration", "org.apache.flink.configuration.GlobalConfiguration.loadYAMLResource", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the configuration files from the specified directory. If the dynamic properties\n * configuration is not null, then it is added to the loaded configuration.\n *\n * @param configDir\n * \t\tdirectory to load the configuration from\n * @param dynamicProperties\n * \t\tconfiguration file containing the dynamic properties. Null if none.\n * @return The configuration loaded from the given configuration directory\n */\npublic static Configuration loadConfiguration(final String configDir, @Nullable\nfinal Configuration dynamicProperties) {\n    if (configDir == null) {\n        throw new IllegalArgumentException(\"Given configuration directory is null, cannot load configuration\");\n    }\n    final File confDirFile = new File(configDir);\n    if (!confDirFile.exists()) {\n        throw new IllegalConfigurationException((((\"The given configuration directory name '\" + configDir) + \"' (\") + confDirFile.getAbsolutePath()) + \") does not describe an existing directory.\");\n    }\n    // get Flink yaml configuration file\n    Configuration configuration;\n    File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);\n    if (!yamlConfigFile.exists()) {\n        throw new IllegalConfigurationException((((\"The Flink config file '\" + yamlConfigFile) + \"' (\") + yamlConfigFile.getAbsolutePath()) + \") does not exist.\");\n    } else {\n        LOG.info(\"Using standard YAML parser to load flink configuration file from {}.\", yamlConfigFile.getAbsolutePath());\n        configuration = loadYAMLResource(yamlConfigFile);\n    }\n    logConfiguration(\"Loading\", configuration);\n    if (dynamicProperties != null) {\n        logConfiguration(\"Loading dynamic\", dynamicProperties);\n        configuration.addAll(dynamicProperties);\n    }\n    return configuration;\n}", "/**\n * Loads a YAML-file of key-value pairs.\n *\n * <p>Keys can be expressed either as nested keys or as {@literal KEY_SEPARATOR} separated keys.\n * For example, the following configurations are equivalent:\n *\n * <pre>\n * jobmanager.rpc.address: localhost # network address for communication with the job manager\n * jobmanager.rpc.port   : 6123      # network port to connect to for communication with the job manager\n * taskmanager.rpc.port  : 6122      # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * <pre>\n * jobmanager:\n *     rpc:\n *         address: localhost # network address for communication with the job manager\n *         port: 6123         # network port to connect to for communication with the job manager\n * taskmanager:\n *     rpc:\n *         port: 6122         # network port the task manager expects incoming IPC connections\n * </pre>\n *\n * @param file\n * \t\tthe YAML file to read from\n * @see <a href=\"http://www.yaml.org/spec/1.2/spec.html\">YAML 1.2 specification</a>\n */\nprivate static Configuration loadYAMLResource(File file) {\n    final Configuration config = new Configuration();\n    try {\n        Map<String, Object> configDocument = flatten(YamlParserUtils.loadYamlFile(file));\n        configDocument.forEach((k, v) -> config.setValueInternal(k, v, false));\n        return config;\n    } catch (Exception e) {\n        throw new RuntimeException(\"Error parsing YAML configuration.\", e);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertToObject",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertToObject", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static synchronized <T> T convertToObject(String value, Class<T> type) {\n    try {\n        return type.cast(loader.loadFromString(value));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.convertAndDumpYamlFromFlatMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts a flat map into a nested map structure and outputs the result as a list of\n * YAML-formatted strings. Each item in the list represents a single line of the YAML data. The\n * method is synchronized and thus thread-safe.\n *\n * @param flattenMap\n * \t\tA map containing flattened keys (e.g., \"parent.child.key\") associated with\n * \t\ttheir values.\n * @return A list of strings that represents the YAML data, where each item corresponds to a\nline of the data.\n */\n@SuppressWarnings(\"unchecked\")\npublic static synchronized List<String> convertAndDumpYamlFromFlatMap(Map<String, Object> flattenMap) {\n    try {\n        Map<String, Object> nestedMap = new LinkedHashMap<>();\n        for (Map.Entry<String, Object> entry : flattenMap.entrySet()) {\n            String[] keys = entry.getKey().split(\"\\\\.\");\n            Map<String, Object> currentMap = nestedMap;\n            for (int i = 0; i < (keys.length - 1); i++) {\n                currentMap = ((Map<String, Object>) (currentMap.computeIfAbsent(keys[i], k -> new LinkedHashMap<>())));\n            }\n            currentMap.put(keys[keys.length - 1], entry.getValue());\n        }\n        String data = blockerDumper.dumpToString(nestedMap);\n        String linebreak = blockerDumperSettings.getBestLineBreak();\n        return Arrays.asList(data.split(linebreak));\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.DelegatingConfiguration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@Override\npublic Map<String, String> toFileWritableMap() {\n    Map<String, String> map = backingConfig.toFileWritableMap();\n    Map<String, String> prefixed = new HashMap<>();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n        if (entry.getKey().startsWith(prefix)) {\n            String keyWithoutPrefix = entry.getKey().substring(prefix.length());\n            prefixed.put(keyWithoutPrefix, YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n    }\n    return prefixed;\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertConfigToWritableLines", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the provided configuration data into a format suitable for writing to a file, based\n * on the {@code flattenYaml} flag and the {@code standardYaml} attribute of the configuration\n * object.\n *\n * <p>Only when {@code flattenYaml} is set to {@code false} and the configuration object is\n * standard yaml, a nested YAML format is used. Otherwise, a flat key-value pair format is\n * output.\n *\n * <p>Each entry in the returned list represents a single line that can be written directly to a\n * file.\n *\n * <p>Example input (flat map configuration data):\n *\n * <pre>{@code {\n *      \"parent.child\": \"value1\",\n *      \"parent.child2\": \"value2\"\n * }}</pre>\n *\n * <p>Example output when {@code flattenYaml} is {@code false} and the configuration object is\n * standard yaml:\n *\n * <pre>{@code parent:\n *   child: value1\n *   child2: value2}</pre>\n *\n * <p>Otherwise, the Example output is:\n *\n * <pre>{@code parent.child: value1\n * parent.child2: value2}</pre>\n *\n * @param configuration\n * \t\tThe configuration to be converted.\n * @param flattenYaml\n * \t\tA boolean flag indicating if the configuration data should be output in a\n * \t\tflattened format.\n * @return A list of strings, where each string represents a line of the file-writable data in\nthe chosen format.\n */\npublic static List<String> convertConfigToWritableLines(Configuration configuration, boolean flattenYaml) {\n    if (!flattenYaml) {\n        return YamlParserUtils.convertAndDumpYamlFromFlatMap(Collections.unmodifiableMap(configuration.confData));\n    } else {\n        Map<String, String> fileWritableMap = configuration.toFileWritableMap();\n        return fileWritableMap.entrySet().stream().map(entry -> (entry.getKey() + \": \") + entry.getValue()).collect(Collectors.toList());\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.loadYamlFile",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.loadYamlFile", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Loads the contents of the given YAML file into a map.\n *\n * @param file\n * \t\tthe YAML file to load.\n * @return a non-null map representing the YAML content. If the file is empty or only contains\ncomments, an empty map is returned.\n * @throws FileNotFoundException\n * \t\tif the YAML file is not found.\n * @throws YamlEngineException\n * \t\tif the file cannot be parsed.\n * @throws IOException\n * \t\tif an I/O error occurs while reading from the file stream.\n */\n@Nonnull\npublic static synchronized Map<String, Object> loadYamlFile(File file) throws Exception {\n    try (FileInputStream inputStream = new FileInputStream(file)) {\n        Map<String, Object> yamlResult = ((Map<String, Object>) (loader.loadFromInputStream(inputStream)));\n        return yamlResult == null ? new HashMap<>() : yamlResult;\n    } catch (FileNotFoundException e) {\n        LOG.error(\"Failed to find YAML file\", e);\n        throw e;\n    } catch (IOException | YamlEngineException e) {\n        if (e instanceof MarkedYamlEngineException) {\n            YamlEngineException exception = wrapExceptionToHiddenSensitiveData(((MarkedYamlEngineException) (e)));\n            LOG.error(\"Failed to parse YAML configuration\", exception);\n            throw exception;\n        } else {\n            throw e;\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseMapToString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseMapToString", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "public static String parseMapToString(Map<String, String> map) {\n    return convertToString(map);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toMap", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n@Override\npublic Map<String, String> toMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            ret.put(entry.getKey(), ConfigurationUtils.convertToString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.YamlParserUtils.toYAMLString",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.YamlParserUtils.toYAMLString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Converts the given value to a string representation in the YAML syntax. This method uses a\n * YAML parser to convert the object to YAML format.\n *\n * <p>The resulting YAML string may have line breaks at the end of each line. This method\n * removes the line break at the end of the string if it exists.\n *\n * <p>Note: This method may perform escaping on certain characters in the value to ensure proper\n * YAML syntax.\n *\n * @param value\n * \t\tThe value to be converted.\n * @return The string representation of the value in YAML syntax.\n */\npublic static synchronized String toYAMLString(Object value) {\n    try {\n        String output = flowDumper.dumpToString(value);\n        // remove the line break\n        String linebreak = flowDumperSettings.getBestLineBreak();\n        if (output.endsWith(linebreak)) {\n            output = output.substring(0, output.length() - linebreak.length());\n        }\n        return output;\n    } catch (MarkedYamlEngineException exception) {\n        throw wrapExceptionToHiddenSensitiveData(exception);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertToList",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertToList", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic static <T> T convertToList(Object rawValue, Class<?> atomicClass) {\n    if (rawValue instanceof List) {\n        return ((T) (rawValue));\n    } else {\n        try {\n            List<Object> data = YamlParserUtils.convertToObject(rawValue.toString(), List.class);\n            // The Yaml parser conversion results in data of type List<Map<Object, Object>>,\n            // such as List<Map<Object, Boolean>>. However, ConfigOption currently requires that\n            // the data for Map type be strictly of the type Map<String, String>. Therefore, we\n            // convert each map in the list to Map<String, String>.\n            if (atomicClass == Map.class) {\n                return ((T) (data.stream().map(map -> convertToStringMap(((Map<Object, Object>) (map)))).collect(Collectors.toList())));\n            }\n            return ((T) (data.stream().map(s -> convertValue(s, atomicClass)).collect(Collectors.toList())));\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToListWithLegacyProperties(rawValue, atomicClass);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.convertValue",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.convertValue", "org.apache.flink.configuration.ConfigurationUtils.convertToString", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n// Type conversion\n// --------------------------------------------------------------------------------------------\n/**\n * Tries to convert the raw value into the provided type.\n *\n * @param rawValue\n * \t\trawValue to convert into the provided type clazz\n * @param clazz\n * \t\tclazz specifying the target type\n * @param <T>\n * \t\ttype of the result\n * @return the converted value if rawValue is of type clazz\n * @throws IllegalArgumentException\n * \t\tif the rawValue cannot be converted in the specified target\n * \t\ttype clazz\n */\n@SuppressWarnings(\"unchecked\")\npublic static <T> T convertValue(Object rawValue, Class<?> clazz) {\n    if (Integer.class.equals(clazz)) {\n        return ((T) (convertToInt(rawValue)));\n    } else if (Long.class.equals(clazz)) {\n        return ((T) (convertToLong(rawValue)));\n    } else if (Boolean.class.equals(clazz)) {\n        return ((T) (convertToBoolean(rawValue)));\n    } else if (Float.class.equals(clazz)) {\n        return ((T) (convertToFloat(rawValue)));\n    } else if (Double.class.equals(clazz)) {\n        return ((T) (convertToDouble(rawValue)));\n    } else if (String.class.equals(clazz)) {\n        return ((T) (convertToString(rawValue)));\n    } else if (clazz.isEnum()) {\n        return ((T) (convertToEnum(rawValue, ((Class<? extends Enum<?>>) (clazz)))));\n    } else if (clazz == Duration.class) {\n        return ((T) (convertToDuration(rawValue)));\n    } else if (clazz == MemorySize.class) {\n        return ((T) (convertToMemorySize(rawValue)));\n    } else if (clazz == Map.class) {\n        return ((T) (convertToProperties(rawValue)));\n    }\n    throw new IllegalArgumentException(\"Unsupported type: \" + clazz);\n}", "static String convertToString(Object o) {\n    if (o.getClass() == String.class) {\n        return ((String) (o));\n    } else {\n        return YamlParserUtils.toYAMLString(o);\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.ConfigurationUtils.parseStringToMap", "org.apache.flink.configuration.ConfigurationUtils.convertToProperties", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Parses a string as a map of strings. The expected format of the map to be parsed` by FLINK\n * parser is:\n *\n * <pre>\n * key1:value1,key2:value2\n * </pre>\n *\n * <p>The expected format of the map to be parsed by standard YAML parser is:\n *\n * <pre>\n * {key1: value1, key2: value2}\n * </pre>\n *\n * <p>Parts of the string can be escaped by wrapping with single or double quotes.\n *\n * @param stringSerializedMap\n * \t\ta string to parse\n * @return parsed map\n */\npublic static Map<String, String> parseStringToMap(String stringSerializedMap) {\n    return convertToProperties(stringSerializedMap);\n}", "@SuppressWarnings(\"unchecked\")\nstatic Map<String, String> convertToProperties(Object o) {\n    if (o instanceof Map) {\n        return ((Map<String, String>) (o));\n    } else {\n        try {\n            Map<Object, Object> map = YamlParserUtils.convertToObject(o.toString(), Map.class);\n            return convertToStringMap(map);\n        } catch (Exception e) {\n            // Fallback to legacy pattern\n            return convertToPropertiesWithLegacyPattern(o);\n        }\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.configuration.Configuration.toFileWritableMap",
    "thirdPartyMethod" : "org.snakeyaml.engine.v2.api.DumpSettings.builder",
    "thirdPartyPackage" : "org.snakeyaml.engine.v2.api",
    "path" : [ "org.apache.flink.configuration.Configuration.toFileWritableMap", "org.apache.flink.configuration.YamlParserUtils.<clinit>" ],
    "fullMethods" : [ "/**\n * Convert Config into a {@code Map<String, String>} representation.\n *\n * <p>NOTE: This method is extracted from the {@link Configuration#toMap} method and should be\n * called when Config needs to be written to a file.\n *\n * <p>This method ensures the value is properly escaped when writing the key-value pair to a\n * standard YAML file.\n */\n@Internal\npublic Map<String, String> toFileWritableMap() {\n    synchronized(this.confData) {\n        Map<String, String> ret = CollectionUtil.newHashMapWithExpectedSize(this.confData.size());\n        for (Map.Entry<String, Object> entry : confData.entrySet()) {\n            // Because some character in standard yaml should be escaped by quotes, such as\n            // '*', here we should wrap the value by Yaml pattern\n            ret.put(entry.getKey(), YamlParserUtils.toYAMLString(entry.getValue()));\n        }\n        return ret;\n    }\n}", "" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setRegistrationRequired",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setRegistrationRequired",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setRegistrationRequired",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setRegistrationRequired",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setRegistrationRequired",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.setRegistrationRequired",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.ObjectMap.put",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.util.ObjectMap.put",
    "thirdPartyPackage" : "com.esotericsoftware.kryo.util",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.operators.util.JoinHashMap.Prober.lookupMatch",
    "thirdPartyMethod" : "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.getValue",
    "thirdPartyPackage" : "org.apache.commons.collections.map.AbstractHashedMap",
    "path" : [ "org.apache.flink.api.common.operators.util.JoinHashMap.Prober.lookupMatch" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic BT lookupMatch(PT record) {\n    int hashCode = hash(probeComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    pairComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && pairComparator.equalToReference(((BT) (entry.getValue())))) {\n            return ((BT) (entry.getValue()));\n        }\n        entry = entryNext(entry);\n    } \n    return null;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace",
    "thirdPartyMethod" : "org.apache.commons.collections.map.AbstractHashedMap.HashEntry.getValue",
    "thirdPartyPackage" : "org.apache.commons.collections.map.AbstractHashedMap",
    "path" : [ "org.apache.flink.api.common.operators.util.JoinHashMap.insertOrReplace" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\npublic void insertOrReplace(BT record) {\n    int hashCode = hash(buildComparator.hash(record));\n    int index = hashIndex(hashCode, data.length);\n    buildComparator.setReference(record);\n    HashEntry entry = data[index];\n    while (entry != null) {\n        if ((entryHashCode(entry) == hashCode) && buildComparator.equalToReference(((BT) (entry.getValue())))) {\n            entry.setValue(record);\n            return;\n        }\n        entry = entryNext(entry);\n    } \n    addMapping(index, hashCode, null, record);\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getGraphContext",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.read" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic T read(Kryo kryo, Input input, Class aClass) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectInputStream objectStream = ((ObjectInputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            // make sure we use Kryo's classloader\n            objectStream = new InstantiationUtil.ClassLoaderObjectInputStream(input, kryo.getClassLoader());\n            graphContext.put(this, objectStream);\n        }\n        return ((T) (objectStream.readObject()));\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java deserialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.getGraphContext",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.JavaSerializer.write" ],
    "fullMethods" : [ "@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n@Override\npublic void write(Kryo kryo, Output output, T o) {\n    try {\n        ObjectMap graphContext = kryo.getGraphContext();\n        ObjectOutputStream objectStream = ((ObjectOutputStream) (graphContext.get(this)));\n        if (objectStream == null) {\n            objectStream = new ObjectOutputStream(output);\n            graphContext.put(this, objectStream);\n        }\n        objectStream.writeObject(o);\n        objectStream.flush();\n    } catch (Exception ex) {\n        throw new KryoException(\"Error during Java serialization.\", ex);\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.readObject",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.common.operators.util.JoinHashMap.<init>",
    "thirdPartyMethod" : "org.apache.commons.collections.map.AbstractHashedMap.<init>",
    "thirdPartyPackage" : "org.apache.commons.collections.map",
    "path" : [ "org.apache.flink.api.common.operators.util.JoinHashMap.<init>" ],
    "fullMethods" : [ "public JoinHashMap(TypeSerializer<BT> buildSerializer, TypeComparator<BT> buildComparator) {\n    super(64);\n    this.buildSerializer = buildSerializer;\n    this.buildComparator = buildComparator;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.Utils.getSerializerTree",
    "thirdPartyMethod" : "org.apache.commons.lang3.StringUtils.repeat",
    "thirdPartyPackage" : "org.apache.commons.lang3",
    "path" : [ "org.apache.flink.util.Utils.getSerializerTree", "org.apache.flink.util.Utils.getSerializerTree" ],
    "fullMethods" : [ "// --------------------------------------------------------------------------------------------\n/**\n * Debugging utility to understand the hierarchy of serializers created by the Java API. Tested\n * in GroupReduceITCase.testGroupByGenericType()\n */\npublic static <T> String getSerializerTree(TypeInformation<T> ti) {\n    return getSerializerTree(ti, 0);\n}", "private static <T> String getSerializerTree(TypeInformation<T> ti, int indent) {\n    String ret = \"\";\n    if (ti instanceof CompositeType) {\n        ret += (StringUtils.repeat(' ', indent) + ti.getClass().getSimpleName()) + \"\\n\";\n        CompositeType<T> cti = ((CompositeType<T>) (ti));\n        String[] fieldNames = cti.getFieldNames();\n        for (int i = 0; i < cti.getArity(); i++) {\n            TypeInformation<?> fieldType = cti.getTypeAt(i);\n            ret += ((StringUtils.repeat(' ', indent + 2) + fieldNames[i]) + \":\") + getSerializerTree(fieldType, indent);\n        }\n    } else if (ti instanceof GenericTypeInfo) {\n        ret += ((StringUtils.repeat(' ', indent) + \"GenericTypeInfo (\") + ti.getTypeClass().getSimpleName()) + \")\\n\";\n        ret += getGenericTypeTree(ti.getTypeClass(), indent + 4);\n    } else {\n        ret += (StringUtils.repeat(' ', indent) + ti.toString()) + \"\\n\";\n    }\n    return ret;\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.writeClassAndObject",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.createInstance", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic T createInstance() {\n    if (Modifier.isAbstract(type.getModifiers()) || Modifier.isInterface(type.getModifiers())) {\n        return null;\n    } else {\n        checkKryoInitialized();\n        try {\n            return kryo.newInstance(type);\n        } catch (Throwable e) {\n            return null;\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void copy(DataInputView source, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (this.copyInstance == null) {\n            this.copyInstance = createInstance();\n        }\n        T tmp = deserialize(copyInstance, source);\n        serialize(tmp, target);\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T deserialize(DataInputView source) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (source != previousIn) {\n            DataInputViewStream inputStream = new DataInputViewStream(source);\n            input = new NoFetchingInput(inputStream);\n            previousIn = source;\n        }\n        try {\n            return ((T) (kryo.readClassAndObject(input)));\n        } catch (KryoBufferUnderflowException ke) {\n            // 2023-04-26: Existing Flink code expects a java.io.EOFException in this scenario\n            throw new EOFException(ke.getMessage());\n        } catch (KryoException ke) {\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@Override\npublic void serialize(T record, DataOutputView target) throws IOException {\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        if (target != previousOut) {\n            DataOutputViewStream outputStream = new DataOutputViewStream(target);\n            output = new Output(outputStream);\n            previousOut = target;\n        }\n        // Sanity check: Make sure that the output is cleared/has been flushed by the last call\n        // otherwise data might be written multiple times in case of a previous EOFException\n        if (output.position() != 0) {\n            throw new IllegalStateException(\"The Kryo Output still contains data from a previous \" + \"serialize call. It has to be flushed or cleared at the end of the serialize call.\");\n        }\n        try {\n            kryo.writeClassAndObject(output, record);\n            output.flush();\n        } catch (KryoException ke) {\n            // make sure that the Kryo output buffer is reset in case that we can recover from\n            // the exception (e.g. EOFException which denotes buffer full)\n            output.reset();\n            Throwable cause = ke.getCause();\n            if (cause instanceof EOFException) {\n                throw ((EOFException) (cause));\n            } else {\n                throw ke;\n            }\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryo", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@VisibleForTesting\npublic Kryo getKryo() {\n    checkKryoInitialized();\n    return this.kryo;\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy",
    "thirdPartyMethod" : "com.esotericsoftware.kryo.Kryo.addDefaultSerializer",
    "thirdPartyPackage" : "com.esotericsoftware.kryo",
    "path" : [ "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy", "org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized" ],
    "fullMethods" : [ "@SuppressWarnings(\"unchecked\")\n@Override\npublic T copy(T from) {\n    if (from == null) {\n        return null;\n    }\n    if (CONCURRENT_ACCESS_CHECK) {\n        enterExclusiveThread();\n    }\n    try {\n        checkKryoInitialized();\n        try {\n            return kryo.copy(from);\n        } catch (KryoException ke) {\n            // kryo was unable to copy it, so we do it through serialization:\n            ByteArrayOutputStream baout = new ByteArrayOutputStream();\n            Output output = new Output(baout);\n            kryo.writeObject(output, from);\n            output.close();\n            ByteArrayInputStream bain = new ByteArrayInputStream(baout.toByteArray());\n            Input input = new Input(bain);\n            return ((T) (kryo.readObject(input, from.getClass())));\n        }\n    } finally {\n        if (CONCURRENT_ACCESS_CHECK) {\n            exitExclusiveThread();\n        }\n    }\n}", "private void checkKryoInitialized() {\n    if (this.kryo == null) {\n        this.kryo = getKryoInstance();\n        // Enable reference tracking.\n        kryo.setReferences(true);\n        // Throwable and all subclasses should be serialized via java serialization\n        // Note: the registered JavaSerializer is Flink's own implementation, and not Kryo's.\n        // This is due to a know issue with Kryo's JavaSerializer. See FLINK-6025 for\n        // details.\n        kryo.addDefaultSerializer(Throwable.class, new JavaSerializer());\n        // Add default serializers first, so that the type registrations without a serializer\n        // are registered with a default serializer\n        for (Map.Entry<Class<?>, SerializableSerializer<?>> entry : defaultSerializers.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue().getSerializer());\n        }\n        for (Map.Entry<Class<?>, Class<? extends Serializer<?>>> entry : defaultSerializerClasses.entrySet()) {\n            kryo.addDefaultSerializer(entry.getKey(), entry.getValue());\n        }\n        KryoUtils.applyRegistrations(this.kryo, kryoRegistrations.values(), flinkChillPackageRegistrar != null ? flinkChillPackageRegistrar.getNextRegistrationId() : kryo.getNextRegistrationId());\n        kryo.setRegistrationRequired(false);\n        kryo.setClassLoader(Thread.currentThread().getContextClassLoader());\n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>",
    "thirdPartyPackage" : "org.apache.commons.compress.compressors.gzip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractTarFile",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.tar.TarArchiveInputStream.close",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.tar",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractTarFile", "org.apache.flink.util.CompressionUtils.extractTarFileUsingJava" ],
    "fullMethods" : [ "public static void extractTarFile(String inFilePath, String targetDirPath) throws IOException {\n    final File targetDir = new File(targetDirPath);\n    if (!targetDir.mkdirs()) {\n        if (!targetDir.isDirectory()) {\n            throw new IOException(\"Mkdirs failed to create \" + targetDir);\n        }\n    }\n    final boolean gzipped = inFilePath.endsWith(\"gz\");\n    if (isUnix()) {\n        extractTarFileUsingTar(inFilePath, targetDirPath, gzipped);\n    } else {\n        extractTarFileUsingJava(inFilePath, targetDirPath, gzipped);\n    }\n}", "// Follow the pattern suggested in\n// https://commons.apache.org/proper/commons-compress/examples.html\nprivate static void extractTarFileUsingJava(String inFilePath, String targetDirPath, boolean gzipped) throws IOException {\n    try (InputStream fi = Files.newInputStream(Paths.get(inFilePath));InputStream bi = new BufferedInputStream(fi);final TarArchiveInputStream tai = new TarArchiveInputStream(gzipped ? new GzipCompressorInputStream(bi) : bi)) {\n        final File targetDir = new File(targetDirPath);\n        TarArchiveEntry entry;\n        while ((entry = tai.getNextTarEntry()) != null) {\n            unpackEntry(tai, entry, targetDir);\n        } \n    }\n}" ]
  }, {
    "entryPoint" : "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions",
    "thirdPartyMethod" : "org.apache.commons.compress.archivers.zip.ZipArchiveEntry.isUnixSymlink",
    "thirdPartyPackage" : "org.apache.commons.compress.archivers.zip",
    "path" : [ "org.apache.flink.util.CompressionUtils.extractZipFileWithPermissions" ],
    "fullMethods" : [ "public static void extractZipFileWithPermissions(String zipFilePath, String targetPath) throws IOException {\n    try (ZipFile zipFile = new ZipFile(zipFilePath)) {\n        Enumeration<ZipArchiveEntry> entries = zipFile.getEntries();\n        boolean isUnix = isUnix();\n        ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        String canonicalTargetPath = new File(targetPath).getCanonicalPath() + File.separator;\n        while (entries.hasMoreElements()) {\n            ZipArchiveEntry entry = entries.nextElement();\n            File outputFile = new File(canonicalTargetPath, entry.getName());\n            if (!outputFile.getCanonicalPath().startsWith(canonicalTargetPath)) {\n                throw new IOException(((\"Expand \" + entry.getName()) + \" would create a file outside of \") + targetPath);\n            }\n            if (entry.isDirectory()) {\n                if (!outputFile.exists()) {\n                    if (!outputFile.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n            } else {\n                File parentDir = outputFile.getParentFile();\n                if (!parentDir.exists()) {\n                    if (!parentDir.mkdirs()) {\n                        throw new IOException((\"Create dir: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                    }\n                }\n                if (entry.isUnixSymlink()) {\n                    // the content of the file is the target path of the symlink\n                    baos.reset();\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), baos);\n                    Files.createSymbolicLink(outputFile.toPath(), new File(parentDir, baos.toString()).toPath());\n                } else if (outputFile.createNewFile()) {\n                    OutputStream output = new FileOutputStream(outputFile);\n                    IOUtils.copyBytes(zipFile.getInputStream(entry), output);\n                } else {\n                    throw new IOException((\"Create file: \" + outputFile.getAbsolutePath()) + \" failed!\");\n                }\n            }\n            if (isUnix) {\n                int mode = entry.getUnixMode();\n                if (mode != 0) {\n                    Path outputPath = Paths.get(outputFile.toURI());\n                    Set<PosixFilePermission> permissions = new HashSet<>();\n                    addIfBitSet(mode, 8, permissions, PosixFilePermission.OWNER_READ);\n                    addIfBitSet(mode, 7, permissions, PosixFilePermission.OWNER_WRITE);\n                    addIfBitSet(mode, 6, permissions, PosixFilePermission.OWNER_EXECUTE);\n                    addIfBitSet(mode, 5, permissions, PosixFilePermission.GROUP_READ);\n                    addIfBitSet(mode, 4, permissions, PosixFilePermission.GROUP_WRITE);\n                    addIfBitSet(mode, 3, permissions, PosixFilePermission.GROUP_EXECUTE);\n                    addIfBitSet(mode, 2, permissions, PosixFilePermission.OTHERS_READ);\n                    addIfBitSet(mode, 1, permissions, PosixFilePermission.OTHERS_WRITE);\n                    addIfBitSet(mode, 0, permissions, PosixFilePermission.OTHERS_EXECUTE);\n                    // the permission of the target file will be set to be the same as the\n                    // symlink\n                    // TODO: support setting the permission without following links\n                    try {\n                        Files.setPosixFilePermissions(outputPath, permissions);\n                    } catch (NoSuchFileException e) {\n                        // this may happens when the target file of the symlink is still not\n                        // extracted\n                    }\n                }\n            }\n        } \n    }\n}" ]
  } ]
}