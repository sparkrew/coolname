{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0046469-ec00-4f6c-8d67-3020442ae782",
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pipeline Setup \n",
    "# %pip install torch torchvision torchaudio transformers sentencepiece accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295ba3b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324dc70-aadc-412b-ac23-5d8a42b52d99",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper Utilities\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9._\\- ]+\", \"_\", s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return s.strip(\"_\")[:80] or \"dependency\"\n",
    "\n",
    "def write_file(path: Path, content: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(content, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad51ea9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model Loader\n",
    "\n",
    "def load_model(model_name: str = \"codellama/CodeLlama-7b-Instruct-hf\"):\n",
    "    \"\"\"Load CodeLlama instruct model for local inference.\"\"\"\n",
    "    print(f\"[info] Loading model: {model_name}\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "    )\n",
    "    print(f\"[info] Model loaded successfully on {device}\")\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ca5ba-fd47-4b07-b443-4fed696a5b01",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model(model_name=\"codellama/CodeLlama-7b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d923044-aab1-45ae-839c-ff82d5eabcfb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test Generation Function\n",
    "\n",
    "def test_generator(\n",
    "    prompt: str,\n",
    "    dependency_data: dict,\n",
    "    model_name: str = \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    output_root: str = \"execution_results\",\n",
    "):\n",
    "    # create folder/files to save model output later\n",
    "    timestamp = datetime.now(datetime.timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    dep_safe = safe_name(dependency_data[\"thirdPartyPackage\"])\n",
    "    folder = Path(output_root) / f\"{dep_safe}_{timestamp}\"\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metadata = {\n",
    "        \"entry_point\": dependency_data[\"entryPoint\"],\n",
    "        \"third_party_method\": dependency_data[\"thirdPartyMethod\"],\n",
    "        \"third_party_package\": dependency_data[\"thirdPartyPackage\"],\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model\": model_name\n",
    "    }\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "    \tmessages,\n",
    "    \tadd_generation_prompt=False,\n",
    "    \ttokenize=True,\n",
    "    \treturn_dict=True,\n",
    "    \treturn_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    print(\"[info] Generating message...\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=4096)\n",
    "    decoded = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]) \n",
    "    print(\"[info] Generation complete. \\n\")\n",
    "\n",
    "\n",
    "    # Clean output\n",
    "    result_text = decoded.replace(prompt, \"\").strip()\n",
    "    m = re.search(r\"```(?:java)?\\s*(.*?)\\s*```\", result_text, re.DOTALL)\n",
    "    java_code = m.group(1).strip() if m else result_text\n",
    "\n",
    "    # Save files\n",
    "    java_filename = f\"{dep_safe}_GeneratedTest.java\"\n",
    "    write_file(folder / \"prompt.txt\", prompt)\n",
    "    write_file(folder / java_filename, java_code)\n",
    "    write_file(folder / \"metadata.json\", json.dumps(metadata, indent=2))\n",
    "\n",
    "    print(f\"[done] Output written to {folder.resolve()}\")\n",
    "    \n",
    "    return {\n",
    "        \"folder\": str(folder.resolve()),\n",
    "        \"java_file\": str((folder / java_filename).resolve()),\n",
    "        \"prompt_file\": str((folder / 'prompt.txt').resolve()),\n",
    "        \"metadata_file\": str((folder / 'metadata.json').resolve()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349f0e8-54bc-4c3c-b95b-bc8b441bac9d",
   "metadata": {},
   "source": [
    "## Sample Usage of The Test Generator on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90154a-246a-4b34-b797-45497bf4a36c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# You can change any of these to try different prompting techniques, but note 'dependency_data' must remain a dictionary.\n",
    "\n",
    "llm_instruction = \"Generate a complete JUnit 5 test suite that exercises the chain of methods starting from the entry point entryPoint, and ensures that the third-party method thirdPartyMethod from thirdPartyPackage is invoked and tested. Include tests for both successful execution and error scenarios. Use Mockito where appropriate. Output only the Java test code, without any explanations or text outside the code block.\"\n",
    "\n",
    "dependency_data_definitions = \"\"\"\n",
    "    Dependency Information Meaning:\n",
    "    - Entry point: path to the method in the project that eventually calls the third-party dependency.\n",
    "    - Entry point body: the entire code block of the entry point method\n",
    "    - Third-party method: the external method that should be tested.\n",
    "    - Third-party package: the package containing the third-party method.\n",
    "    - Path: the sequence of method calls from entry point to the third-party method.\n",
    "    - Full methods: the full source code of the relevant methods from the entry point to the dependecy.\n",
    "    - Method Slices: the code slices of all relevant methods from the entry point to the dependency\n",
    "\"\"\"\n",
    "\n",
    "# example dependency data\n",
    "dependency_data = {\n",
    "    \"entryPoint\" : \"com.graphhopper.util.GHUtility.loadCustomModelFromJar\",\n",
    "    \"thirdPartyMethod\" : \"com.fasterxml.jackson.databind.ObjectMapper.readValue\",\n",
    "    \"thirdPartyPackage\" : \"com.fasterxml.jackson.databind\",\n",
    "    \"path\" : [ \"com.graphhopper.util.GHUtility.loadCustomModelFromJar\", \"com.fasterxml.jackson.databind.ObjectMapper.readValue\" ],\n",
    "    \"fullMethods\" : [\n",
    "      \"public static CustomModel loadCustomModelFromJar(String name) {\\n        try {\\n            InputStream is = GHUtility.class.getResourceAsStream(\\\"/com/graphhopper/custom_models/\\\" + name);\\n            if (is == null)\\n                throw new IllegalArgumentException(\\\"There is no built-in custom model '\\\" + name + \\\"'\\\");\\n            String json = readJSONFileWithoutComments(new InputStreamReader(is));\\n            ObjectMapper objectMapper = Jackson.newObjectMapper();\\n            return objectMapper.readValue(json, CustomModel.class);\\n        } catch (IOException e) {\\n            throw new IllegalArgumentException(\\\"Could not load built-in custom model '\\\" + name + \\\"'\\\", e);\\n        }\\n    }\",\n",
    "      \"/**\\n     * Method to deserialize JSON content from given JSON content String.\\n     *\\n     * @throws StreamReadException if underlying input contains invalid content\\n     *    of type {@link JsonParser} supports (JSON for default case)\\n     * @throws DatabindException if the input JSON structure does not match structure\\n     *   expected for result type (or has other mismatch issues)\\n     */\\n    public <T> T readValue(String content, Class<T> valueType)\\n        throws JsonProcessingException, JsonMappingException\\n    {\\n        _assertNotNull(\\\"content\\\", content);\\n        return readValue(content, _typeFactory.constructType(valueType));\\n    }\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "llm_output_instruction = \"\"\"\n",
    "    - Please produce a single Java file (JUnit 5) with imports, test methods, and short comments explaining each test briefly. Use Mockito for mocking InputStream or ObjectMapper where appropriate. Ensure the test class can compile in a typical Maven/Gradle project.\n",
    "    - Output only valid Java test code inside a ```java``` block.\n",
    "    - Do not include any explanations or markdown outside the code block and do not repeat the propmt back.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab6227-f75c-45a6-9eb7-01a0e9add013",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Concatenate prompt\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Instructions\n",
    "{llm_instruction}\n",
    "\n",
    "Dependency Data Definitions\n",
    "{dependency_data_definitions}\n",
    "\n",
    "Dependency Data\n",
    "{dependency_data}\n",
    "\n",
    "Output Instructions\n",
    "{llm_output_instruction}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d161c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate test\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     results = test_generator(\n",
    "#         prompt,\n",
    "#         dependency_data\n",
    "#     )\n",
    "\n",
    "#     print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ffd35-780e-45fe-ac82-c92be49065ec",
   "metadata": {},
   "source": [
    "# Automating test generation for etheo code examples \n",
    "\n",
    " - ensure to upload the example JSON files from etheo github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6414586-963e-4fbf-bd4f-fd83b190224a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _build_dependency_data_from_example(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize example object to the dependency_data shape expected by test_generator.\n",
    "    Falls back sensibly when fields are missing.\n",
    "    \"\"\"\n",
    "    dep = {}\n",
    "    dep[\"entryPoint\"] = example.get(\"entryPoint\") or \"UNKNOWN_ENTRY_POINT\"\n",
    "    dep[\"thirdPartyMethod\"] = example.get(\"thirdPartyMethod\") or \"UNKNOWN_THIRDPARTY_METHOD\"\n",
    "    dep[\"thirdPartyPackage\"] = example.get(\"thirdPartyPackage\") or \"UNKNOWN_PACKAGE\"\n",
    "    \n",
    "    if \"fullMethods\" in example and isinstance(example[\"fullMethods\"], list):\n",
    "        dep[\"fullMethods\"] = example[\"fullMethods\"] or []\n",
    "    elif \"methodSlices\" in example and isinstance(example[\"methodSlices\"], list):\n",
    "        dep[\"methodSlices\"] = example.get(\"methodSlices\") or []\n",
    "    else:\n",
    "        dep[\"entryPointBody\"] = example.get(\"entryPointBody\") or []\n",
    "        \n",
    "    dep[\"path\"] = example.get(\"path\") or []\n",
    "    \n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e281ce7-2452-48a9-bb31-a0c32b3c1af3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_etheo_example_tests(\n",
    "    examples_dir: str,\n",
    "    output_root: str = \"execution_results_batch\",\n",
    "    model_name: str = \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    max_examples: int = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reads JSON example files from examples_dir,\n",
    "    for each example build a prompt using the notebook's prompting pieces and call the\n",
    "    existing `test_generator` function.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    examples_path = Path(examples_dir)\n",
    "    if not examples_path.exists():\n",
    "        raise FileNotFoundError(f\"Examples directory not found: {examples_dir}\")\n",
    "\n",
    "    json_files = sorted(examples_path.glob(\"*.json\"))\n",
    "    processed = 0\n",
    "\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            raw = jf.read_text(encoding=\"utf-8\")\n",
    "            parsed = json.loads(raw)\n",
    "            if not isinstance(parsed, list):\n",
    "                # tolerate a top-level object containing an array under a key\n",
    "                if isinstance(parsed, dict):\n",
    "                    # try to find the first list value\n",
    "                    lists = [v for v in parsed.values() if isinstance(v, list)]\n",
    "                    parsed = lists[0] if lists else []\n",
    "            if not parsed:\n",
    "                print(f\"[skip] {jf.name} - no examples found\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Failed to read/parse {jf.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for idx, example in enumerate(parsed):\n",
    "            if max_examples is not None and processed >= max_examples:\n",
    "                print(\"[info] reached max_examples limit\")\n",
    "                return results\n",
    "\n",
    "            try:\n",
    "                dependency_data = _build_dependency_data_from_example(example)\n",
    "                dep_json = json.dumps(dependency_data, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                prompt = (\n",
    "                    f\"Instructions\\n\"\n",
    "                    f\"{llm_instruction}\\n\\n\"\n",
    "                    f\"Dependency Data Definitions\\n\"\n",
    "                    f\"{dependency_data_definitions}\\n\\n\"\n",
    "                    f\"Dependency Data\\n\"\n",
    "                    f\"{dep_json}\\n\\n\"\n",
    "                    f\"Output Instructions\\n\"\n",
    "                    f\"{llm_output_instruction}\\n\"\n",
    "                )\n",
    "\n",
    "                out = test_generator(\n",
    "                    prompt=prompt,\n",
    "                    dependency_data=dependency_data,\n",
    "                    model_name=model_name,\n",
    "                    output_root=output_root,\n",
    "                )\n",
    "\n",
    "                results.append({\n",
    "                    \"source_file\": jf.name,\n",
    "                    \"example_index\": idx,\n",
    "                    \"entryPoint\": dependency_data.get(\"entryPoint\"),\n",
    "                    \"thirdPartyMethod\": dependency_data.get(\"thirdPartyMethod\"),\n",
    "                    \"result\": out,\n",
    "                })\n",
    "                processed += 1\n",
    "                print(f\"[ok] {jf.name} #{idx} -> {dependency_data.get('thirdPartyMethod')} \\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[error] {jf.name} #{idx} failed: {e}\")\n",
    "                continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a1d48-30ca-4ad7-9623-bf23db6c0f45",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = generate_etheo_example_tests(\n",
    "    examples_dir = \"/examples\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8524075,
     "sourceId": 13430157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
